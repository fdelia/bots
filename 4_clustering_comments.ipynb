{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships between words\n",
    "### n-grams and correlations\n",
    "Read: http://tidytextmining.com/ngrams.html\n",
    "\n",
    "Exploring relationships and connections between words.\n",
    "1. Tokenizing by n-gram, here by bigrams for a start\n",
    "2. Counting and filtering n-grams: Most common bigrams, then remove the ones where at least one is a stop word.\n",
    "3. Analyzing bigrams: one bigram per row: look at the tf-idf (the one with the highest), can be visualized for each document(/book in example).\n",
    "4. Using them to provide context in sentiment analysis: The approach is to count certain word segments like \"happy\" and \"not happy\". We examine how often sentiment-associated words are preceded by \"not\" or other negating words. We use the `AFINN lexicon for sentiment analysis`. Is there something similar in german for sentiment analysis? -> http://www.ulliwaltinger.de/sentiment/ (gives negative, positive and neutral words in tsv-format, not sure if the words have a \"sentiment score\")\n",
    "5. Calculate sentiment score per comment\n",
    "6. Visualize a network of bigrams: node1 = word1 -> node2 = word2 of bigram, weight given by the number of occurence of the bigram. For each word we only show the words that follows it the most as connected directed node. Only show bigrams that occured at least x times.  \n",
    "An useful and flexibel way to visualize relational data. - This is a visualization of a Markov chain.\n",
    "7. Put the whole thing built so far into a function for usage on other texts\n",
    "8. We may be interested in words that tend to co-occur within particular documents or particular chapters, even if they don't occur next to each other. Turn text into a wide matrix first for that.  \n",
    "Counting and correlating among sections: Package like `widyr` in R for that? (`pairwise_count`)\n",
    "9. Most common co-occuring words not that meaningful since they're also the most common individual words. We may instead want to examine correlation among words, which indicates how often they appear together relative to how often they appear separately (phi coefficient ~ pearson correlation for binary data).  \n",
    "Pick some interesting words and look at their correlations. Then plot a graph for highest correlations like above.\n",
    "    \n",
    "Pairs of consecutive words might capture structure that isn't present when one is just counting single words, and may provide context that makes tokens more understandable (for example \"pulteney street\" instead of \"pulteney\" only). However, the per-bigram counts are also sparser since they are rarer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize libraries and data\n",
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df_art = pd.read_csv('articles_2017_08.csv')\n",
    "df_com = pd.read_csv('comments_2017_08.csv').sample(5000) # crop because battery life, skews data\n",
    "# Make float better readable\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# https://de.wikipedia.org/wiki/Liste_der_h%C3%A4ufigsten_W%C3%B6rter_der_deutschen_Sprache\n",
    "stop_words = \"die, der, und, in, zu, den, das, nicht, von, sie, ist, des, sich, mit, dem, dass, er, es, ein, ich, auf, so, eine, auch, als, an, nach, wie, im, für\"\n",
    "stop_words += \"man, aber, aus, durch, wenn, nur, war, noch, werden, bei, hat, wir, was, wird, sein, einen, welche, sind, oder, zur, um, haben, einer, mir, über, ihm, diese, einem, ihr, uns\"\n",
    "#stop_words += \"da, zum, kann, doch, vor, dieser, mich, ihn, du, hatte, seine, mehr, am, denn, nun, unter, sehr, selbst, schon, hier\"\n",
    "#stop_words += \"bis, habe, ihre, dann, ihnen, seiner, alle, wieder, meine, Zeit, gegen, vom, ganz, einzelnen, wo, muss, ohne, eines, können, sei\"\n",
    "stop_words = stop_words.lower()\n",
    "stop_words = stop_words.split(', ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Topic modeling\n",
    "Read: http://tidytextmining.com/topicmodeling.html\n",
    "\n",
    "Use LDA: Each document a mixture of topics and each topic a mixture of words. Built in package?\n",
    "\n",
    "It's possible to declare what a \"document\" is. Also look at how each document is classified (per topic). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words=stop_words)\n",
    "tfidf = tfidf_vectorizer.fit_transform(df_com['con'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
