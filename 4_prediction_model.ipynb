{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction of User Reactions\n",
    "We want to predict:\n",
    "* How to get high score/consensus (upvotes - downvotes)\n",
    "* How to be controversial (upvotes + downvotes)  \n",
    "in the comments.\n",
    "\n",
    "I won't:\n",
    "* Determine the average score of a user/author and use this as information. Because this is not helpful once we have a user with no earlier comments.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Not necessary, but I like the ggplot style better\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df_art = pd.read_csv('articles_2017_09.csv')\n",
    "df_com = pd.read_csv('comments_2017_09.csv')#.sample(100000) # crop because battery life\n",
    "# Make float better readable\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tId</th>\n",
       "      <th>art_vup_max</th>\n",
       "      <th>art_vup_min</th>\n",
       "      <th>art_vup_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280671</th>\n",
       "      <td>10003016</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>166.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280662</th>\n",
       "      <td>10003016</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>166.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280668</th>\n",
       "      <td>10003016</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>166.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280666</th>\n",
       "      <td>10003016</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>166.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280655</th>\n",
       "      <td>10003016</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>166.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280663</th>\n",
       "      <td>10003016</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>166.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280664</th>\n",
       "      <td>10003016</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>166.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280667</th>\n",
       "      <td>10003016</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>166.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280672</th>\n",
       "      <td>10003016</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>166.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280670</th>\n",
       "      <td>10003016</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>166.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tId  art_vup_max  art_vup_min  art_vup_mean\n",
       "280671  10003016          602            0       166.667\n",
       "280662  10003016          602            0       166.667\n",
       "280668  10003016          602            0       166.667\n",
       "280666  10003016          602            0       166.667\n",
       "280655  10003016          602            0       166.667\n",
       "280663  10003016          602            0       166.667\n",
       "280664  10003016          602            0       166.667\n",
       "280667  10003016          602            0       166.667\n",
       "280672  10003016          602            0       166.667\n",
       "280670  10003016          602            0       166.667"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_dt_obj(time):\n",
    "    time = time.replace('am ', '')\n",
    "    # Make datetime object from string\n",
    "    return datetime.strptime(time, '%d.%m.%Y %H:%M')\n",
    "\n",
    "def time_since_epoch(time):\n",
    "    return (get_dt_obj(time)-datetime(1970,1,1)).total_seconds()\n",
    "\n",
    "def get_hour_of_day(time):\n",
    "    return get_dt_obj(time).hour\n",
    "\n",
    "def get_weekday(time):\n",
    "    return get_dt_obj(time).weekday()\n",
    "\n",
    "# Basically same as \"the hour of week\" or \"weekday_hourOfDay\"\n",
    "def get_weekday_float(time):\n",
    "    hour = float(get_hour_of_day(time))\n",
    "    weekday = get_weekday(time)\n",
    "    return float(weekday) + hour / 24\n",
    "\n",
    "def get_weekday_hour(time):\n",
    "    return '{}_{}'.format(get_weekday(time), get_hour_of_day(time))\n",
    "\n",
    "df_com['time_since_epoch'] = df_com['time'].apply(time_since_epoch)\n",
    "df_com['hour'] = df_com['time'].apply(get_hour_of_day)\n",
    "df_com['weekday'] = df_com['time'].apply(get_weekday) # 0 = Monday\n",
    "df_com['weekday_fl'] = df_com['time'].apply(get_weekday_float)\n",
    "#df_com['weekday_hour'] = df_com['time'].apply(get_weekday_hour)\n",
    "df_com['is_answer'] = df_com['tit'].apply(lambda x: str(x).startswith('@'))\n",
    "df_com['con_len'] = df_com['con'].apply(lambda x: len(x))\n",
    "df_com['con_num_words'] = df_com['con'].apply(lambda x: len(x.split()))\n",
    "df_com['score'] = df_com['vup'] - df_com['vdo']\n",
    "df_com['contr'] = df_com['vup'] + df_com['vdo']\n",
    "\n",
    "df_com['tit'] = df_com['tit'].str.lower()\n",
    "df_com['con'] = df_com['con'].str.lower()\n",
    "\n",
    "def get_category(link):\n",
    "    t = link.split('/')\n",
    "    if len(t) <= 1:\n",
    "        return ''\n",
    "    else:\n",
    "        return t[1]\n",
    "\n",
    "df_art['cat'] = df_art['link'].apply(get_category)\n",
    "df_art['cat_copy'] = df_art['cat']\n",
    "df_art = pd.get_dummies(df_art, columns=['cat'])\n",
    "\n",
    "df_art['header_len'] = df_art['header'].apply(lambda x: len(x))\n",
    "df_art['text_len'] = df_art['text'].apply(lambda x: len(str(x)))\n",
    "df_art['text_num_words'] = df_art['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Left inner join\n",
    "df_merge = pd.merge(left=df_com, right=df_art, left_on='tId', right_on='tId')\n",
    "\n",
    "# Remove rows with missing values\n",
    "# use .count() to check for missing values\n",
    "df_merge.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# Get order of comments per article\n",
    "df_merge = df_merge.sort_values(['tId', 'time_since_epoch']).groupby('tId')\n",
    "\n",
    "# Get time since the first comment\n",
    "def get_time_since_first(group):\n",
    "    first = group.iloc[:1]['time_since_epoch']\n",
    "    group['time_since_first'] = group['time_since_epoch'].apply(lambda x: (x - first) / 3600)\n",
    "#    group['art_first_weekday']\n",
    "    # Remove those very late comments, after x hours\n",
    "    #group = group[group['time_since_first'] < 36]\n",
    "    \n",
    "    # I'm not sure if this is cheating and discloses too much information\n",
    "    for col in ['vup', 'vdo', 'con_len']:\n",
    "        group['art_' + col + '_max'] = group[col].max()\n",
    "        group['art_' + col + '_min'] = group[col].min()\n",
    "        group['art_' + col + '_mean'] = group[col].mean()\n",
    "    \n",
    "    # TODO add more info here\n",
    "    return group\n",
    "\n",
    "# Creating \"copy\" to make next cell independent\n",
    "df_merge = df_merge.apply(get_time_since_first)\n",
    "df_merge.head(10)[['tId', 'art_vup_max', 'art_vup_min', 'art_vup_mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to remove skew\n",
    "__Rescaling__: Add or subtract a constant and then multiply or divide by a constant.  \n",
    "__Normalizing__: Dividing by a norm of the vector, e.g. make Euclidean length equal to one. Sometimes make all elements lie in [0, 1].  \n",
    "__Standardizing__: Subtracting a measure of location and dividing by a measure of scale. Eg. subtract the mean and divide by the std, thereby obtaining a standard normal distribution.\n",
    "\n",
    "These terms are sometimes used interchangeably.\n",
    "\n",
    "It's usually better to have the input values centered around zero, unless the output activation function has a range of [0, 1] (neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntemp = df_merge[['aut']].groupby('aut').size().reset_index()\\ntemp = temp.rename(columns = {0: 'user_n_comments'})\\n\\ntemp2 = pd.get_dummies(df_merge[['aut', 'weekday']], prefix='wd_', columns=['weekday', 'hour'])\\ntemp = temp.merge(temp2).reset_index()\\ndf = df_merge.merge(temp, on='aut')\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "temp = df_merge[['aut']].groupby('aut').size().reset_index()\n",
    "temp = temp.rename(columns = {0: 'user_n_comments'})\n",
    "\n",
    "temp2 = pd.get_dummies(df_merge[['aut', 'weekday']], prefix='wd_', columns=['weekday', 'hour'])\n",
    "temp = temp.merge(temp2).reset_index()\n",
    "df = df_merge.merge(temp, on='aut')\n",
    "\"\"\"\n",
    "#print(len(df))\n",
    "#df = df_merge.merge(temp2, left_index=True, right_index=True)\n",
    "#print(df.head(2)[['tId', 'weekday', 'wd__1']])\n",
    "#temp2.head(2)\n",
    "#temp2 = pd.get_dummies(df_merge[['cId', 'hour']], prefix='wd_', columns=['hour'])\n",
    "#temp2.head(1)\n",
    "#df = df.merge(temp2, left_index=True, right_index=True) \n",
    "#df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>score_trans</th>\n",
       "      <th>vup</th>\n",
       "      <th>vdo</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday_fl</th>\n",
       "      <th>con_len_trans</th>\n",
       "      <th>text_len_trans</th>\n",
       "      <th>con_num_words_trans</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_schweiz</th>\n",
       "      <th>cat_finance</th>\n",
       "      <th>cat_sport</th>\n",
       "      <th>cat_wissen</th>\n",
       "      <th>cat_ausland</th>\n",
       "      <th>wd__0</th>\n",
       "      <th>wd__1</th>\n",
       "      <th>wd__2</th>\n",
       "      <th>hour__17</th>\n",
       "      <th>hour__18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_trans</th>\n",
       "      <td>0.476</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vup</th>\n",
       "      <td>0.952</td>\n",
       "      <td>0.351</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vdo</th>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.334</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_fl</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.130</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_len_trans</th>\n",
       "      <td>0.013</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len_trans</th>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.069</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_num_words_trans</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_n_comments</th>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_num_words_trans</th>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_comments</th>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_comments_trans</th>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_since_first_trans</th>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>header_len_trans</th>\n",
       "      <td>0.022</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_n_periods</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_schweiz</th>\n",
       "      <td>0.036</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.044</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_finance</th>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_sport</th>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_wissen</th>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_ausland</th>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wd__0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wd__1</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wd__2</th>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour__17</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour__18</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        score  score_trans    vup    vdo  weekday   hour  \\\n",
       "score                   1.000        0.476  0.952  0.031   -0.006 -0.004   \n",
       "score_trans             0.476        1.000  0.351 -0.319   -0.005 -0.007   \n",
       "vup                     0.952        0.351  1.000  0.334   -0.001 -0.000   \n",
       "vdo                     0.031       -0.319  0.334  1.000    0.014  0.012   \n",
       "weekday                -0.006       -0.005 -0.001  0.014    1.000  0.010   \n",
       "hour                   -0.004       -0.007 -0.000  0.012    0.010  1.000   \n",
       "weekday_fl             -0.006       -0.006 -0.001  0.015    0.993  0.130   \n",
       "con_len_trans           0.013        0.034  0.019  0.025   -0.005 -0.049   \n",
       "text_len_trans          0.005       -0.003  0.007  0.009   -0.037 -0.040   \n",
       "con_num_words_trans     0.010        0.030  0.017  0.024   -0.006 -0.049   \n",
       "user_n_comments        -0.014       -0.022 -0.021 -0.026    0.009  0.026   \n",
       "text_num_words_trans    0.006       -0.002  0.008  0.009   -0.044 -0.041   \n",
       "num_comments           -0.031       -0.033 -0.038 -0.028    0.022  0.025   \n",
       "num_comments_trans     -0.015       -0.021 -0.017 -0.011    0.030  0.010   \n",
       "time_since_first_trans -0.178       -0.175 -0.219 -0.166    0.094 -0.072   \n",
       "header_len_trans        0.022        0.016  0.024  0.010   -0.037  0.015   \n",
       "con_n_periods           0.004        0.022  0.004  0.003   -0.008 -0.017   \n",
       "cat_schweiz             0.036        0.024  0.043  0.030   -0.021 -0.040   \n",
       "cat_finance            -0.030       -0.014 -0.044 -0.051   -0.019 -0.016   \n",
       "cat_sport              -0.023       -0.035 -0.017  0.015    0.016  0.018   \n",
       "cat_wissen             -0.016       -0.006 -0.021 -0.021    0.022  0.038   \n",
       "cat_ausland             0.005       -0.008  0.018  0.044   -0.019  0.006   \n",
       "wd__0                   0.001       -0.006  0.002  0.003   -0.565 -0.022   \n",
       "wd__1                   0.016        0.016  0.014 -0.002   -0.425  0.023   \n",
       "wd__2                  -0.018       -0.008 -0.023 -0.019   -0.219 -0.007   \n",
       "hour__17                0.005        0.001  0.007  0.008    0.001  0.159   \n",
       "hour__18                0.008        0.006  0.010  0.010    0.002  0.196   \n",
       "\n",
       "                        weekday_fl  con_len_trans  text_len_trans  \\\n",
       "score                       -0.006          0.013           0.005   \n",
       "score_trans                 -0.006          0.034          -0.003   \n",
       "vup                         -0.001          0.019           0.007   \n",
       "vdo                          0.015          0.025           0.009   \n",
       "weekday                      0.993         -0.005          -0.037   \n",
       "hour                         0.130         -0.049          -0.040   \n",
       "weekday_fl                   1.000         -0.011          -0.041   \n",
       "con_len_trans               -0.011          1.000           0.069   \n",
       "text_len_trans              -0.041          0.069           1.000   \n",
       "con_num_words_trans         -0.012          0.989           0.067   \n",
       "user_n_comments              0.012         -0.082          -0.005   \n",
       "text_num_words_trans        -0.048          0.067           0.994   \n",
       "num_comments                 0.025          0.058           0.138   \n",
       "num_comments_trans           0.031          0.072           0.198   \n",
       "time_since_first_trans       0.085          0.041          -0.053   \n",
       "header_len_trans            -0.035          0.021           0.063   \n",
       "con_n_periods               -0.010          0.536           0.028   \n",
       "cat_schweiz                 -0.025          0.049           0.177   \n",
       "cat_finance                 -0.020          0.033           0.052   \n",
       "cat_sport                    0.018         -0.053          -0.014   \n",
       "cat_wissen                   0.027         -0.008          -0.018   \n",
       "cat_ausland                 -0.018          0.016           0.071   \n",
       "wd__0                       -0.563         -0.013           0.019   \n",
       "wd__1                       -0.419         -0.001          -0.006   \n",
       "wd__2                       -0.218          0.011           0.014   \n",
       "hour__17                     0.020         -0.017          -0.021   \n",
       "hour__18                     0.026         -0.015          -0.017   \n",
       "\n",
       "                        con_num_words_trans    ...     cat_schweiz  \\\n",
       "score                                 0.010    ...           0.036   \n",
       "score_trans                           0.030    ...           0.024   \n",
       "vup                                   0.017    ...           0.043   \n",
       "vdo                                   0.024    ...           0.030   \n",
       "weekday                              -0.006    ...          -0.021   \n",
       "hour                                 -0.049    ...          -0.040   \n",
       "weekday_fl                           -0.012    ...          -0.025   \n",
       "con_len_trans                         0.989    ...           0.049   \n",
       "text_len_trans                        0.067    ...           0.177   \n",
       "con_num_words_trans                   1.000    ...           0.044   \n",
       "user_n_comments                      -0.084    ...          -0.005   \n",
       "text_num_words_trans                  0.065    ...           0.179   \n",
       "num_comments                          0.056    ...           0.116   \n",
       "num_comments_trans                    0.071    ...           0.143   \n",
       "time_since_first_trans                0.042    ...          -0.104   \n",
       "header_len_trans                      0.018    ...           0.092   \n",
       "con_n_periods                         0.529    ...           0.000   \n",
       "cat_schweiz                           0.044    ...           1.000   \n",
       "cat_finance                           0.032    ...          -0.408   \n",
       "cat_sport                            -0.047    ...          -0.249   \n",
       "cat_wissen                           -0.009    ...          -0.246   \n",
       "cat_ausland                           0.012    ...          -0.228   \n",
       "wd__0                                -0.013    ...          -0.018   \n",
       "wd__1                                -0.001    ...           0.020   \n",
       "wd__2                                 0.012    ...           0.038   \n",
       "hour__17                             -0.017    ...           0.002   \n",
       "hour__18                             -0.016    ...           0.008   \n",
       "\n",
       "                        cat_finance  cat_sport  cat_wissen  cat_ausland  \\\n",
       "score                        -0.030     -0.023      -0.016        0.005   \n",
       "score_trans                  -0.014     -0.035      -0.006       -0.008   \n",
       "vup                          -0.044     -0.017      -0.021        0.018   \n",
       "vdo                          -0.051      0.015      -0.021        0.044   \n",
       "weekday                      -0.019      0.016       0.022       -0.019   \n",
       "hour                         -0.016      0.018       0.038        0.006   \n",
       "weekday_fl                   -0.020      0.018       0.027       -0.018   \n",
       "con_len_trans                 0.033     -0.053      -0.008        0.016   \n",
       "text_len_trans                0.052     -0.014      -0.018        0.071   \n",
       "con_num_words_trans           0.032     -0.047      -0.009        0.012   \n",
       "user_n_comments               0.016     -0.022      -0.013        0.011   \n",
       "text_num_words_trans          0.038     -0.003      -0.020        0.062   \n",
       "num_comments                  0.185     -0.153       0.027       -0.101   \n",
       "num_comments_trans            0.195     -0.152       0.014       -0.071   \n",
       "time_since_first_trans        0.022      0.015       0.021        0.019   \n",
       "header_len_trans              0.012     -0.156       0.020        0.001   \n",
       "con_n_periods                 0.023     -0.012       0.001        0.002   \n",
       "cat_schweiz                  -0.408     -0.249      -0.246       -0.228   \n",
       "cat_finance                   1.000     -0.125      -0.123       -0.114   \n",
       "cat_sport                    -0.125      1.000      -0.075       -0.070   \n",
       "cat_wissen                   -0.123     -0.075       1.000       -0.069   \n",
       "cat_ausland                  -0.114     -0.070      -0.069        1.000   \n",
       "wd__0                        -0.049      0.036      -0.002        0.014   \n",
       "wd__1                         0.019     -0.029      -0.019       -0.001   \n",
       "wd__2                         0.005     -0.021       0.018        0.004   \n",
       "hour__17                     -0.023     -0.000       0.005        0.011   \n",
       "hour__18                     -0.021      0.001       0.005        0.003   \n",
       "\n",
       "                        wd__0  wd__1  wd__2  hour__17  hour__18  \n",
       "score                   0.001  0.016 -0.018     0.005     0.008  \n",
       "score_trans            -0.006  0.016 -0.008     0.001     0.006  \n",
       "vup                     0.002  0.014 -0.023     0.007     0.010  \n",
       "vdo                     0.003 -0.002 -0.019     0.008     0.010  \n",
       "weekday                -0.565 -0.425 -0.219     0.001     0.002  \n",
       "hour                   -0.022  0.023 -0.007     0.159     0.196  \n",
       "weekday_fl             -0.563 -0.419 -0.218     0.020     0.026  \n",
       "con_len_trans          -0.013 -0.001  0.011    -0.017    -0.015  \n",
       "text_len_trans          0.019 -0.006  0.014    -0.021    -0.017  \n",
       "con_num_words_trans    -0.013 -0.001  0.012    -0.017    -0.016  \n",
       "user_n_comments         0.002 -0.002 -0.012     0.006     0.006  \n",
       "text_num_words_trans    0.024 -0.004  0.014    -0.022    -0.018  \n",
       "num_comments           -0.033  0.000  0.061    -0.008    -0.011  \n",
       "num_comments_trans     -0.038 -0.020  0.054    -0.007    -0.006  \n",
       "time_since_first_trans  0.010 -0.076 -0.044    -0.017    -0.005  \n",
       "header_len_trans       -0.003  0.017  0.012     0.008     0.013  \n",
       "con_n_periods          -0.005  0.005  0.004    -0.010    -0.004  \n",
       "cat_schweiz            -0.018  0.020  0.038     0.002     0.008  \n",
       "cat_finance            -0.049  0.019  0.005    -0.023    -0.021  \n",
       "cat_sport               0.036 -0.029 -0.021    -0.000     0.001  \n",
       "cat_wissen             -0.002 -0.019  0.018     0.005     0.005  \n",
       "cat_ausland             0.014 -0.001  0.004     0.011     0.003  \n",
       "wd__0                   1.000 -0.156 -0.176    -0.009    -0.009  \n",
       "wd__1                  -0.156  1.000 -0.204    -0.006     0.004  \n",
       "wd__2                  -0.176 -0.204  1.000     0.008    -0.002  \n",
       "hour__17               -0.009 -0.006  0.008     1.000    -0.057  \n",
       "hour__18               -0.009  0.004 -0.002    -0.057     1.000  \n",
       "\n",
       "[27 rows x 27 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#temp2 = pd.get_dummies(temp[(temp['user_n_comments']<100) & (temp['user_n_comments']>50)], prefix='user', columns=['aut'])\n",
    "#print(\"users with certain amount of comments: \", len(temp2))\n",
    "temp2 = pd.get_dummies(df_merge[['cId', 'weekday']], prefix='wd_', columns=['weekday'])\n",
    "# Merge here first because of index!\n",
    "df = df_merge.merge(temp2, left_index=True, right_index=True) \n",
    "temp2 = pd.get_dummies(df_merge[['cId', 'hour']], prefix='hour_', columns=['hour'])\n",
    "df = df.merge(temp2, left_index=True, right_index=True) \n",
    "\n",
    "# User based information\n",
    "temp = df_merge[['aut']].groupby('aut').size().reset_index()\n",
    "temp = temp.rename(columns = {0: 'user_n_comments'})\n",
    "df = df.merge(temp, on='aut')\n",
    "\n",
    "df['con_n_periods'] = df['con'].apply(lambda x: len(x.split('.')))\n",
    "\n",
    "def sgn(x):\n",
    "    if x == 0: return 0\n",
    "    else: return x/abs(x)\n",
    "# Removes left/right skew \n",
    "for col in ['weekday_fl', 'con_len', 'text_len', 'time_since_first', \n",
    "            'num_comments', 'user_n_comments', 'con_n_periods',\n",
    "            'score', 'contr', 'header_len', 'con_num_words', 'text_num_words']:\n",
    "    df[col + '_trans'] = df[col].apply(lambda x: sgn(x)*math.log(abs(x) + 1))\n",
    "\n",
    "# Memory optimization\n",
    "# Technical stuff, contributes nothing to analysis\n",
    "conv = df.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='unsigned')\n",
    "df[conv.columns] = conv\n",
    "    \n",
    "# I removed very weak correlations to \"score\" and \"contr\"\n",
    "cols = ['score', 'score_trans', 'vup', 'vdo', 'weekday', 'hour', 'weekday_fl',\n",
    "       'con_len_trans', 'text_len_trans', 'con_num_words_trans', 'user_n_comments',\n",
    "        'text_num_words_trans', 'num_comments', 'num_comments_trans',\n",
    "       'time_since_first_trans', 'header_len_trans', 'con_n_periods',\n",
    "        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "       'wd__0', 'wd__1', 'wd__2', 'hour__17', 'hour__18']#'user_Amina123', 'user_Chris', 'user_Dani']\n",
    "# Get pearson co-efficients\n",
    "df[cols].corr()\n",
    "\n",
    "#df.hist('user_score_mean_trans')\n",
    "\n",
    "#for c in cols:\n",
    "#    print(c)\n",
    "#    print(df[np.isnan(df[col])].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and split for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data:  287500\n"
     ]
    }
   ],
   "source": [
    "# https://de.wikipedia.org/wiki/Liste_der_h%C3%A4ufigsten_W%C3%B6rter_der_deutschen_Sprache\n",
    "stop_words = \"die, der, und, in, zu, den, das, nicht, von, sie, ist, des, sich, mit, dem, dass, er, es, ein, ich, auf, so, eine, auch, als, an, nach, wie, im, für, \"\n",
    "stop_words += \"man, aber, aus, durch, wenn, nur, war, noch, werden, bei, hat, wir, was, wird, sein, einen, welche, sind, oder, zur, um, haben, einer, mir, über, ihm, diese, einem, ihr, uns, \"\n",
    "stop_words += \"da, zum, kann, doch, vor, dieser, mich, ihn, du, hatte, seine, mehr, am, denn, nun, unter, sehr, selbst, schon, hier, \"\n",
    "stop_words += \"bis, habe, ihre, dann, ihnen, seiner, alle, wieder, meine, Zeit, gegen, vom, ganz, einzelnen, wo, muss, ohne, eines, können, sei\"\n",
    "stop_words = stop_words.lower()\n",
    "stop_words = stop_words.split(', ')\n",
    "\n",
    "X = df.drop(['score', 'contr', 'vup', 'vdo'], axis=1)\n",
    "y = df['score']\n",
    "\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "print(\"total data: \", len(X))\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, cv=None, ylim=None, n_jobs=1, train_sizes=np.linspace(.5, 1.0, 2)):\n",
    "    plt.figure()\n",
    "    plt.title('Learning curve')\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel('Training examples'); plt.ylabel('Score')\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, explained_variance_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Select a subset of data at a provided key.\n",
    "    key: hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    '''\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.key]\n",
    "    \n",
    "ngram_range = (1, 1)\n",
    "cat_cols = [col for col in list(df.columns) if col.startswith('wd_') or col.startswith('art_') or col.startswith('hour_')]\n",
    "st = time()\n",
    "def no_numbers(x):\n",
    "    return re.sub(r'(\\d[\\d\\.])+', '', x.lower())\n",
    "model = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        [\n",
    "            # comment + user information\n",
    "            ('statistics', Pipeline([\n",
    "                ('selector', ItemSelector(\n",
    "                    key=['con_len_trans', 'num_comments', \n",
    "                         'time_since_first_trans',# 'con_num_words',\n",
    "                          'user_n_comments_trans', #'con_n_periods_trans',\n",
    "                        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "                         'cat_panorama', 'cat_community', 'cat_people', 'cat_digital'\n",
    "                   #     ])),\n",
    "                        ] + cat_cols)),\n",
    "              #  ('scaler', StandardScaler()),\n",
    "             #   ('reduce_dim', PCA()),\n",
    "            ])),\n",
    "\n",
    "            ('words_content', Pipeline([\n",
    "                ('selector', ItemSelector(key='con')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range, stop_words=stop_words)),\n",
    "             #   ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            ('words_title', Pipeline([\n",
    "                ('selector', ItemSelector(key='tit')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range, stop_words=stop_words)),\n",
    "              #  ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            # article information\n",
    "            ('words_header', Pipeline([\n",
    "                ('selector', ItemSelector(key='header')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range, stop_words=stop_words)),\n",
    "              #  ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            ('words_subheader', Pipeline([\n",
    "                ('selector', ItemSelector(key='sub')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range, stop_words=stop_words)),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            ('words_text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=0.2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "               # ('best', TruncatedSVD(n_components=20)),\n",
    "            ])),\n",
    "        ], n_jobs=-1\n",
    "    )),\n",
    "    ('reduce_features', SelectKBest(f_regression, k=50)),\n",
    "    #('reduce_dim', TruncatedSVD(n_components=100)),\n",
    "    ('model', MLPRegressor(max_iter=30, hidden_layer_sizes=(100,)))\n",
    "    #('model', GradientBoostingRegressor(n_estimators=100, learning_rate=1.5, max_depth=3))\n",
    "])\n",
    "\n",
    "# Just choose what you want to do\n",
    "if True:\n",
    "    plot_learning_curve(model, X, y, cv=2, n_jobs=-1)\n",
    "    plt.show()\n",
    "\n",
    "if True:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"R^2: %1.3f\" % r2_score(y_test, y_pred))\n",
    "    #print(\"Explained var: {:3f}\".format(explained_variance_score(y_test, y_pred)))\n",
    "    print(\"Mean absolute err: {:3f}\".format(mean_absolute_error(y_test, y_pred)))\n",
    "    print(\"Mean squared err: {:3f}\".format(mean_squared_error(y_test, y_pred)))\n",
    "    # Residual plot\n",
    "    # time_since_first because it seems to have some influence (see feature importance below)\n",
    "    X_res = X_test['time_since_first_trans']\n",
    "    plt.scatter(X_res, y_test, color='black', label='test data')\n",
    "    plt.scatter(X_res, y_pred, color='red', label='predicted')\n",
    "    plt.xlabel('time_since_first_trans')\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "if False:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_grid = {\n",
    "        'model__hidden_layer_sizes':[(50,), (100,), (200,), (100, 50)]\n",
    "#        'model__alpha': np.logspace(-1, 4, 6),\n",
    "                 }\n",
    "    grid = GridSearchCV(model, param_grid, cv=3, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "print('Done in {:2f} min.'.format((time() - st)/60))\n",
    "# Make yourself noticeable to human\n",
    "import os; os.system('say \"this is dave: your program has finished.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary so far\n",
    "Neural networks: R^2 = 0.39 (old try)  \n",
    "GBRF: R^2 = 0.45  \n",
    "ngram range (1, 2): 0.37  ngram range (1, 1): 0.366   \n",
    "adding more stop words: gives a small increase\n",
    "\n",
    "So far: still high variance, overall score not that good.\n",
    "\n",
    "The models improve slightly with more data. The problem is that I may not have enough data yet (250k comments).\n",
    "\n",
    "TODO:  \n",
    "How to extract more information? How to use user behaviour? Use collaborative filtering?  \n",
    "Save trained model, make a prediction function where it's easy to input data to try around with score prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['con_len_trans', 'num_comments', 'time_since_first_trans', 'user_n_comments_trans', 'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland', 'cat_panorama', 'cat_community', 'cat_people', 'cat_digital', 'art_vup_max', 'art_vup_min', 'art_vup_mean', 'art_vdo_max', 'art_vdo_min', 'art_vdo_mean', 'art_con_len_max', 'art_con_len_min', 'art_con_len_mean', 'wd__0', 'wd__1', 'wd__2', 'wd__3', 'wd__4', 'wd__5', 'wd__6', 'hour__0', 'hour__1', 'hour__2', 'hour__3', 'hour__4', 'hour__5', 'hour__6', 'hour__7', 'hour__8', 'hour__9', 'hour__10', 'hour__11', 'hour__12', 'hour__13', 'hour__14', 'hour__15', 'hour__16', 'hour__17', 'hour__18', 'hour__19', 'hour__20', 'hour__21', 'hour__22', 'hour__23']\n",
      "0\n",
      "content\n",
      "[]\n",
      "tit\n",
      "[]\n",
      "sub\n",
      "[  4.07977434e-01   1.10340081e-01   2.89355434e-02   1.33963602e-01\n",
      "   4.71128344e-02   1.64685038e-02   7.21665989e-02   1.77734221e-02\n",
      "   3.35282865e-03   0.00000000e+00   9.59980109e-03   1.20942076e-02\n",
      "   0.00000000e+00   3.40846765e-04   1.09780726e-02   7.83942549e-03\n",
      "   5.30387026e-03   3.00088895e-03   7.89758659e-03   7.56802470e-03]\n",
      "text\n",
      "[ 0.00575338  0.0100981   0.00603664  0.01231492  0.01112279  0.01181771\n",
      "  0.          0.          0.00112595  0.          0.00136825  0.00192726\n",
      "  0.00023985  0.00167746  0.00129655  0.          0.          0.00250891\n",
      "  0.00362495  0.00076561  0.00229666  0.00548163  0.00282912  0.00067931\n",
      "  0.00071953  0.00484528  0.          0.          0.001895    0.00686155]\n"
     ]
    }
   ],
   "source": [
    "# Try to find out which features are not important\n",
    "# It seems that the text features are rather important\n",
    "\n",
    "fi = model.named_steps['model'].feature_importances_\n",
    "\"\"\"\n",
    "print(['con_len_trans', 'num_comments', \n",
    "                         'time_since_first_trans',# 'con_num_words',\n",
    "                          'user_n_comments_trans', #'con_n_periods_trans',\n",
    "                        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "                         'cat_panorama', 'cat_community', 'cat_people', 'cat_digital'\n",
    "                        ] + cat_cols)\n",
    "#print(fi)\n",
    "\"\"\"\n",
    "print(len(fi[:-120]))\n",
    "print('content')\n",
    "print(fi[-120:-90])\n",
    "print('tit')\n",
    "print(fi[-90:-60])\n",
    "print('sub')\n",
    "print(fi[-60:-30])\n",
    "print('text')\n",
    "print(fi[-30:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
