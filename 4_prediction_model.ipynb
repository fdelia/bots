{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction of User Reactions\n",
    "We want to predict:\n",
    "* How to get high score/consensus (upvotes - downvotes)\n",
    "* How to be controversial (upvotes + downvotes)  \n",
    "in the comments.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Not necessary, but I like the ggplot style better\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df_art = pd.read_csv('articles_2017_09.csv')\n",
    "df_com = pd.read_csv('comments_2017_09.csv').sample(150000) # crop because battery life\n",
    "# Make float better readable\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_dt_obj(time):\n",
    "    time = time.replace('am ', '')\n",
    "    # Make datetime object from string\n",
    "    return datetime.strptime(time, '%d.%m.%Y %H:%M')\n",
    "\n",
    "def time_since_epoch(time):\n",
    "    return (get_dt_obj(time)-datetime(1970,1,1)).total_seconds()\n",
    "\n",
    "def get_hour_of_day(time):\n",
    "    return get_dt_obj(time).hour\n",
    "\n",
    "def get_weekday(time):\n",
    "    return get_dt_obj(time).weekday()\n",
    "\n",
    "# Basically same as \"the hour of week\" or \"weekday_hourOfDay\"\n",
    "def get_weekday_float(time):\n",
    "    hour = float(get_hour_of_day(time))\n",
    "    weekday = get_weekday(time)\n",
    "    return float(weekday) + hour / 24\n",
    "\n",
    "def get_weekday_hour(time):\n",
    "    return '{}_{}'.format(get_weekday(time), get_hour_of_day(time))\n",
    "\n",
    "df_com['time_since_epoch'] = df_com['time'].apply(time_since_epoch)\n",
    "df_com['hour'] = df_com['time'].apply(get_hour_of_day)\n",
    "df_com['weekday'] = df_com['time'].apply(get_weekday) # 0 = Monday\n",
    "df_com['weekday_fl'] = df_com['time'].apply(get_weekday_float)\n",
    "#df_com['weekday_hour'] = df_com['time'].apply(get_weekday_hour)\n",
    "df_com['is_answer'] = df_com['tit'].apply(lambda x: str(x).startswith('@'))\n",
    "df_com['con_len'] = df_com['con'].apply(lambda x: len(x))\n",
    "df_com['con_num_words'] = df_com['con'].apply(lambda x: len(x.split()))\n",
    "df_com['score'] = df_com['vup'] - df_com['vdo']\n",
    "df_com['contr'] = df_com['vup'] + df_com['vdo']\n",
    "\n",
    "df_com['tit'] = df_com['tit'].str.lower()\n",
    "df_com['con'] = df_com['con'].str.lower()\n",
    "\n",
    "def get_category(link):\n",
    "    t = link.split('/')\n",
    "    if len(t) <= 1:\n",
    "        return ''\n",
    "    else:\n",
    "        return t[1]\n",
    "\n",
    "df_art['cat'] = df_art['link'].apply(get_category)\n",
    "df_art['cat_copy'] = df_art['cat']\n",
    "df_art = pd.get_dummies(df_art, columns=['cat'])\n",
    "\n",
    "df_art['header_len'] = df_art['header'].apply(lambda x: len(x))\n",
    "df_art['text_len'] = df_art['text'].apply(lambda x: len(str(x)))\n",
    "df_art['text_num_words'] = df_art['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Left inner join\n",
    "df_merge = pd.merge(left=df_com, right=df_art, left_on='tId', right_on='tId')\n",
    "\n",
    "# Remove rows with missing values\n",
    "# use .count() to check for missing values\n",
    "df_merge.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# Get order of comments per article\n",
    "df_merge = df_merge.sort_values(['tId', 'time_since_epoch']).groupby('tId')\n",
    "\n",
    "# Get time since the first comment\n",
    "def get_time_since_first(group):\n",
    "    first = group.iloc[:1]['time_since_epoch']\n",
    "    group['time_since_first'] = group['time_since_epoch'].apply(lambda x: (x - first) / 3600)\n",
    "    # Remove those very late comments, after x hours\n",
    "    #group = group[group['time_since_first'] < 36]\n",
    "    return group\n",
    "\n",
    "# Creating \"copy\" to make next cell independent\n",
    "df_merge = df_merge.apply(get_time_since_first)\n",
    "df_merge.head(3)\n",
    "#df[df.index.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to remove skew\n",
    "__Rescaling__: Add or subtract a constant and then multiply or divide by a constant.  \n",
    "__Normalizing__: Dividing by a norm of the vector, e.g. make Euclidean length equal to one. Sometimes make all elements lie in [0, 1].  \n",
    "__Standardizing__: Subtracting a measure of location and dividing by a measure of scale. Eg. subtract the mean and divide by the std, thereby obtaining a standard normal distribution.\n",
    "\n",
    "These terms are sometimes used interchangeably.\n",
    "\n",
    "It's usually better to have the input values centered around zero, unless the output activation function has a range of [0, 1] (neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#temp = df[['aut', 'score']].groupby('aut').size().reset_index()\n",
    "#temp = temp.rename(columns = {0: 'user_n_comments'})\n",
    "#temp2 = pd.get_dummies(temp[temp['user_n_comments']>60], prefix='user_', columns=['aut'])\n",
    "#temp = temp.merge(temp2)\n",
    "#temp.head(2)\n",
    "#df.head(1)\n",
    "#user_cols = [col for col in list(df.columns) if col.startswith('user_')]\n",
    "#print(user_cols)\n",
    "#df_com[df_com['aut'] == 'Chris'][['aut', 'score']]\n",
    "#list(df.columns)\n",
    "#df.groupby('aut')[['aut', 'score']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# User based information\n",
    "#temp = df_merge[['aut', 'score']].groupby('aut').mean() # <- this is cheating!\n",
    "#temp = temp.rename(columns = {'score': 'user_score_mean'}).reset_index()\n",
    "#df = df_merge.merge(temp, on='aut')\n",
    "temp = df_merge[['aut']].groupby('aut').size().reset_index()\n",
    "temp = temp.rename(columns = {0: 'user_n_comments'})\n",
    "temp2 = pd.get_dummies(temp[temp['user_n_comments']>50], prefix='user', columns=['aut'])\n",
    "print(len(temp2))\n",
    "temp = temp.merge(temp2).reset_index()\n",
    "df = df_merge.merge(temp, on='aut')\n",
    "\n",
    "df['con_n_periods'] = df['con'].apply(lambda x: len(x.split('.')))\n",
    "\n",
    "def sgn(x):\n",
    "    if x == 0: return 0\n",
    "    else: return x/abs(x)\n",
    "# Removes left/right skew \n",
    "for col in ['weekday_fl', 'con_len', 'text_len', 'time_since_first', \n",
    "            'num_comments', 'user_n_comments', 'con_n_periods',\n",
    "            'score', 'contr', 'header_len', 'con_num_words', 'text_num_words']:\n",
    "    df[col + '_trans'] = df[col].apply(lambda x: sgn(x)*math.log(abs(x) + 1))\n",
    "\n",
    "# Memory optimization\n",
    "# Technical stuff, contributes nothing to analysis\n",
    "conv = df.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='unsigned')\n",
    "df[conv.columns] = conv\n",
    "    \n",
    "# I removed very weak correlations to \"score\" and \"contr\"\n",
    "cols = ['score', 'score_trans', 'vup', 'vdo', 'weekday', 'hour', 'weekday_fl',\n",
    "       'con_len_trans', 'text_len_trans', 'con_num_words_trans', 'user_n_comments',\n",
    "        'text_num_words_trans', 'num_comments', 'num_comments_trans',\n",
    "       'time_since_first_trans', 'header_len_trans', 'con_n_periods',\n",
    "        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "       'user_Amina123', 'user_Chris', 'user_Dani']\n",
    "# Get pearson co-efficients\n",
    "df[cols].corr()\n",
    "\n",
    "#df.hist('user_score_mean_trans')\n",
    "\n",
    "#for c in cols:\n",
    "#    print(c)\n",
    "#    print(df[np.isnan(df[col])].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and split for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://de.wikipedia.org/wiki/Liste_der_h%C3%A4ufigsten_W%C3%B6rter_der_deutschen_Sprache\n",
    "stop_words = \"die, der, und, in, zu, den, das, nicht, von, sie, ist, des, sich, mit, dem, dass, er, es, ein, ich, auf, so, eine, auch, als, an, nach, wie, im, für\"\n",
    "stop_words += \"man, aber, aus, durch, wenn, nur, war, noch, werden, bei, hat, wir, was, wird, sein, einen, welche, sind, oder, zur, um, haben, einer, mir, über, ihm, diese, einem, ihr, uns\"\n",
    "#stop_words += \"da, zum, kann, doch, vor, dieser, mich, ihn, du, hatte, seine, mehr, am, denn, nun, unter, sehr, selbst, schon, hier\"\n",
    "#stop_words += \"bis, habe, ihre, dann, ihnen, seiner, alle, wieder, meine, Zeit, gegen, vom, ganz, einzelnen, wo, muss, ohne, eines, können, sei\"\n",
    "stop_words = stop_words.lower()\n",
    "stop_words = stop_words.split(', ')\n",
    "\n",
    "X = df.drop(['score', 'contr', 'vup', 'vdo'], axis=1)\n",
    "y = df['score']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print(\"rows total: \", len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, explained_variance_score, mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Select a subset of data at a provided key.\n",
    "    key: hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    '''\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.key]\n",
    "    \n",
    "ngram_range = (1, 1)\n",
    "user_cols = [col for col in list(df.columns) if col.startswith('user_')]\n",
    "no_numbers = lambda x: re.sub(r'(\\d[\\d\\.])+', '', x.lower())\n",
    "model = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        [\n",
    "            # comment + user information\n",
    "            ('statistics', Pipeline([\n",
    "                ('selector', ItemSelector(\n",
    "                    key=['weekday', 'hour', 'con_len_trans', 'num_comments', \n",
    "                         'time_since_first_trans',# 'con_num_words',\n",
    "                          'user_n_comments_trans', #'con_n_periods_trans',\n",
    "                        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "                         'cat_panorama', 'cat_community', 'cat_people', 'cat_digital'\n",
    "                        ] + user_cols)),\n",
    "                # Polynomialfeatures can help a little bit...\n",
    "#                ('polynomialfeatures', PolynomialFeatures(degree=2)),\n",
    "            ])),\n",
    "\n",
    "            ('words_content', Pipeline([\n",
    "                ('selector', ItemSelector(key='con')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "                ('best', TruncatedSVD(n_components=40)),\n",
    "            ])),\n",
    "            \n",
    "            # article information\n",
    "            ('words_title', Pipeline([\n",
    "                ('selector', ItemSelector(key='tit')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            ('words_subtitle', Pipeline([\n",
    "                ('selector', ItemSelector(key='sub')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            ('words_text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=0.2, max_df=0.7, preprocessor=no_numbers, ngram_range=(1, 2))),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "\n",
    "        ]\n",
    "    )),    \n",
    "    #('model', MLPRegressor(max_iter=50, hidden_layer_sizes=(100,)))\n",
    "    ('model', GradientBoostingRegressor(n_estimators=50, max_depth=3))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R^2: %1.3f\" % r2_score(y_test, y_pred))\n",
    "print(\"Explained var: {:3f}\".format(explained_variance_score(y_test, y_pred)))\n",
    "#print(y_pred)\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Residual plot\n",
    "# time_since_first because it seems to have some influence (see feature importance below)\n",
    "X_res = X_test['time_since_first_trans']\n",
    "plt.scatter(X_res, y_test, color='black', label='test data')\n",
    "plt.scatter(X_res, y_pred, color='red', label='predicted')\n",
    "plt.xlabel('time_since_first_trans')\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "if False:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_grid = {\n",
    "        'model__hidden_layer_sizes':[(50,), (100,), (200,), (100, 50)]\n",
    "#        'motdel__alpha': np.logspace(-1, 4, 6),\n",
    "                 }\n",
    "    grid = GridSearchCV(model, param_grid, cv=3)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary so far\n",
    "The decision tree regressor depends a lot on max_depth parameter: Depending on it, the model over- or underfits. I reached the best R^2 score with max_depth=3, but model underfits.\n",
    "\n",
    "Neural networks work a bit better: R^2 = 0.39 (old try)  \n",
    "GBRF same: R^2 = 0.45\n",
    "ngram range (1, 2): 0.37  \n",
    "ngram range (1, 1): 0.366   \n",
    "\n",
    "The models improve slightly with more data, will try bigger runs.\n",
    "\n",
    "How to extract more information? How to use user behaviour?\n",
    "\n",
    "TODO: better text feature extration. Save trained model, make a prediction function where it's easy to input data to try around with score prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try to find out which features are not important\n",
    "# It seems that the text features are rather important\n",
    "\n",
    "fi = model.named_steps['model'].feature_importances_\n",
    "print(len(['weekday', 'hour', 'con_len_trans', 'num_comments', \n",
    "                         'time_since_first_trans',# 'con_num_words',\n",
    "                          'user_n_comments_trans', #'con_n_periods_trans',\n",
    "                        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "                         'cat_panorama', 'cat_community', 'cat_people', 'cat_digital'\n",
    "                       ] + user_cols))\n",
    "print(len(fi[:-130]))\n",
    "print('content')\n",
    "print(fi[-130:-90])\n",
    "print('tit')\n",
    "print(fi[-90:-60])\n",
    "print('sub')\n",
    "print(fi[-60:-30])\n",
    "print('text')\n",
    "print(fi[-30:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
