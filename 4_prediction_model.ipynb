{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction of User Reactions\n",
    "We want to predict:\n",
    "* How to get high score/consensus (upvotes - downvotes)\n",
    "* How to be controversial (upvotes + downvotes)  \n",
    "in the comments.\n",
    "\n",
    "I won't:\n",
    "* Determine the average score of a user/author and use this as information. Because this is not helpful once we have a user with no earlier comments.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Not necessary, but I like the ggplot style better\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df_art = pd.read_csv('articles_2017_09.csv')\n",
    "df_com = pd.read_csv('comments_2017_09.csv').sample(50000) # crop because battery life\n",
    "# Make float better readable\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_dt_obj(time):\n",
    "    time = time.replace('am ', '')\n",
    "    # Make datetime object from string\n",
    "    return datetime.strptime(time, '%d.%m.%Y %H:%M')\n",
    "\n",
    "def time_since_epoch(time):\n",
    "    return (get_dt_obj(time)-datetime(1970,1,1)).total_seconds()\n",
    "\n",
    "def get_hour_of_day(time):\n",
    "    return get_dt_obj(time).hour\n",
    "\n",
    "def get_weekday(time):\n",
    "    return get_dt_obj(time).weekday()\n",
    "\n",
    "# Basically same as \"the hour of week\" or \"weekday_hourOfDay\"\n",
    "def get_weekday_float(time):\n",
    "    hour = float(get_hour_of_day(time))\n",
    "    weekday = get_weekday(time)\n",
    "    return float(weekday) + hour / 24\n",
    "\n",
    "def get_weekday_hour(time):\n",
    "    return '{}_{}'.format(get_weekday(time), get_hour_of_day(time))\n",
    "\n",
    "df_com['time_since_epoch'] = df_com['time'].apply(time_since_epoch)\n",
    "df_com['hour'] = df_com['time'].apply(get_hour_of_day)\n",
    "df_com['weekday'] = df_com['time'].apply(get_weekday) # 0 = Monday\n",
    "df_com['weekday_fl'] = df_com['time'].apply(get_weekday_float)\n",
    "#df_com['weekday_hour'] = df_com['time'].apply(get_weekday_hour)\n",
    "df_com['is_answer'] = df_com['tit'].apply(lambda x: str(x).startswith('@'))\n",
    "df_com['con_len'] = df_com['con'].apply(lambda x: len(x))\n",
    "df_com['con_num_words'] = df_com['con'].apply(lambda x: len(x.split()))\n",
    "df_com['score'] = df_com['vup'] - df_com['vdo']\n",
    "df_com['contr'] = df_com['vup'] + df_com['vdo']\n",
    "\n",
    "df_com['tit'] = df_com['tit'].str.lower()\n",
    "df_com['con'] = df_com['con'].str.lower()\n",
    "\n",
    "def get_category(link):\n",
    "    t = link.split('/')\n",
    "    if len(t) <= 1:\n",
    "        return ''\n",
    "    else:\n",
    "        return t[1]\n",
    "\n",
    "df_art['cat'] = df_art['link'].apply(get_category)\n",
    "df_art['cat_copy'] = df_art['cat']\n",
    "df_art = pd.get_dummies(df_art, columns=['cat'])\n",
    "\n",
    "df_art['header_len'] = df_art['header'].apply(lambda x: len(x))\n",
    "df_art['text_len'] = df_art['text'].apply(lambda x: len(str(x)))\n",
    "df_art['text_num_words'] = df_art['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Left inner join\n",
    "df_merge = pd.merge(left=df_com, right=df_art, left_on='tId', right_on='tId')\n",
    "\n",
    "# Remove rows with missing values\n",
    "# use .count() to check for missing values\n",
    "df_merge.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# Get order of comments per article\n",
    "df_merge = df_merge.sort_values(['tId', 'time_since_epoch']).groupby('tId')\n",
    "\n",
    "# Get time since the first comment\n",
    "def get_time_since_first(group):\n",
    "    first = group.iloc[:1]['time_since_epoch']\n",
    "    group['time_since_first'] = group['time_since_epoch'].apply(lambda x: (x - first) / 3600)\n",
    "#    group['art_first_weekday']\n",
    "    # Remove those very late comments, after x hours\n",
    "    #group = group[group['time_since_first'] < 36]\n",
    "    \n",
    "    # I'm not sure if this is cheating and discloses too much information\n",
    "    for col in ['vup', 'vdo', 'con_len']:\n",
    "        group['art_' + col + '_max'] = group[col].max()\n",
    "        group['art_' + col + '_min'] = group[col].min()\n",
    "        group['art_' + col + '_mean'] = group[col].mean()\n",
    "    \n",
    "    # TODO add more info here\n",
    "    return group\n",
    "\n",
    "# Creating \"copy\" to make next cell independent\n",
    "df_merge = df_merge.apply(get_time_since_first)\n",
    "df_merge.head(5)[['art_vup_max', 'art_vup_min', 'art_vup_mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to remove skew\n",
    "__Rescaling__: Add or subtract a constant and then multiply or divide by a constant.  \n",
    "__Normalizing__: Dividing by a norm of the vector, e.g. make Euclidean length equal to one. Sometimes make all elements lie in [0, 1].  \n",
    "__Standardizing__: Subtracting a measure of location and dividing by a measure of scale. Eg. subtract the mean and divide by the std, thereby obtaining a standard normal distribution.\n",
    "\n",
    "These terms are sometimes used interchangeably.\n",
    "\n",
    "It's usually better to have the input values centered around zero, unless the output activation function has a range of [0, 1] (neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50693\n",
      "            tId  weekday  wd__1\n",
      "49725  10003016        1      1\n",
      "49724  10003016        1      1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cId</th>\n",
       "      <th>wd__0</th>\n",
       "      <th>wd__1</th>\n",
       "      <th>wd__2</th>\n",
       "      <th>wd__3</th>\n",
       "      <th>wd__4</th>\n",
       "      <th>wd__5</th>\n",
       "      <th>wd__6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49725</th>\n",
       "      <td>25_25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49724</th>\n",
       "      <td>129_129</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           cId  wd__0  wd__1  wd__2  wd__3  wd__4  wd__5  wd__6\n",
       "49725    25_25      0      1      0      0      0      0      0\n",
       "49724  129_129      0      1      0      0      0      0      0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "temp = df_merge[['aut']].groupby('aut').size().reset_index()\n",
    "temp = temp.rename(columns = {0: 'user_n_comments'})\n",
    "\n",
    "temp2 = pd.get_dummies(df_merge[['aut', 'weekday']], prefix='wd_', columns=['weekday', 'hour'])\n",
    "temp = temp.merge(temp2).reset_index()\n",
    "df = df_merge.merge(temp, on='aut')\n",
    "\"\"\"\n",
    "print(len(df))\n",
    "df = df_merge.merge(temp2, left_index=True, right_index=True)\n",
    "print(df.head(2)[['tId', 'weekday', 'wd__1']])\n",
    "temp2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>score_trans</th>\n",
       "      <th>vup</th>\n",
       "      <th>vdo</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday_fl</th>\n",
       "      <th>con_len_trans</th>\n",
       "      <th>text_len_trans</th>\n",
       "      <th>con_num_words_trans</th>\n",
       "      <th>...</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_comments_trans</th>\n",
       "      <th>time_since_first_trans</th>\n",
       "      <th>header_len_trans</th>\n",
       "      <th>con_n_periods</th>\n",
       "      <th>cat_schweiz</th>\n",
       "      <th>cat_finance</th>\n",
       "      <th>cat_sport</th>\n",
       "      <th>cat_wissen</th>\n",
       "      <th>cat_ausland</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_trans</th>\n",
       "      <td>0.475</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vup</th>\n",
       "      <td>0.954</td>\n",
       "      <td>0.348</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vdo</th>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.365</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.011</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_fl</th>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_len_trans</th>\n",
       "      <td>0.017</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len_trans</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.063</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_num_words_trans</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.059</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_n_comments</th>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_num_words_trans</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_comments</th>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.052</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_comments_trans</th>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_since_first_trans</th>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.078</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>header_len_trans</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_n_periods</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_schweiz</th>\n",
       "      <td>0.037</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_finance</th>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_sport</th>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_wissen</th>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_ausland</th>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        score  score_trans    vup    vdo  weekday   hour  \\\n",
       "score                   1.000        0.475  0.954  0.069   -0.003  0.000   \n",
       "score_trans             0.475        1.000  0.348 -0.317   -0.008 -0.004   \n",
       "vup                     0.954        0.348  1.000  0.365    0.001  0.004   \n",
       "vdo                     0.069       -0.317  0.365  1.000    0.011  0.013   \n",
       "weekday                -0.003       -0.008  0.001  0.011    1.000  0.005   \n",
       "hour                    0.000       -0.004  0.004  0.013    0.005  1.000   \n",
       "weekday_fl             -0.003       -0.009  0.001  0.012    0.993  0.124   \n",
       "con_len_trans           0.017        0.037  0.022  0.021    0.002 -0.052   \n",
       "text_len_trans          0.008        0.004  0.011  0.010   -0.042 -0.038   \n",
       "con_num_words_trans     0.015        0.034  0.020  0.020    0.002 -0.051   \n",
       "user_n_comments        -0.016       -0.027 -0.022 -0.024    0.011  0.033   \n",
       "text_num_words_trans    0.009        0.005  0.012  0.010   -0.048 -0.039   \n",
       "num_comments           -0.029       -0.033 -0.036 -0.028    0.018  0.023   \n",
       "num_comments_trans     -0.015       -0.019 -0.018 -0.012    0.025  0.009   \n",
       "time_since_first_trans -0.175       -0.175 -0.215 -0.172    0.094 -0.073   \n",
       "header_len_trans        0.020        0.017  0.022  0.011   -0.030  0.017   \n",
       "con_n_periods           0.005        0.020  0.004 -0.003    0.001 -0.018   \n",
       "cat_schweiz             0.037        0.019  0.045  0.034   -0.024 -0.040   \n",
       "cat_finance            -0.033       -0.015 -0.047 -0.053   -0.023 -0.018   \n",
       "cat_sport              -0.021       -0.034 -0.016  0.013    0.018  0.013   \n",
       "cat_wissen             -0.010       -0.002 -0.015 -0.019    0.029  0.043   \n",
       "cat_ausland             0.006       -0.003  0.020  0.050   -0.020  0.008   \n",
       "\n",
       "                        weekday_fl  con_len_trans  text_len_trans  \\\n",
       "score                       -0.003          0.017           0.008   \n",
       "score_trans                 -0.009          0.037           0.004   \n",
       "vup                          0.001          0.022           0.011   \n",
       "vdo                          0.012          0.021           0.010   \n",
       "weekday                      0.993          0.002          -0.042   \n",
       "hour                         0.124         -0.052          -0.038   \n",
       "weekday_fl                   1.000         -0.004          -0.046   \n",
       "con_len_trans               -0.004          1.000           0.063   \n",
       "text_len_trans              -0.046          0.063           1.000   \n",
       "con_num_words_trans         -0.004          0.989           0.059   \n",
       "user_n_comments              0.015         -0.089          -0.008   \n",
       "text_num_words_trans        -0.053          0.060           0.994   \n",
       "num_comments                 0.021          0.054           0.136   \n",
       "num_comments_trans           0.026          0.071           0.198   \n",
       "time_since_first_trans       0.085          0.049          -0.034   \n",
       "header_len_trans            -0.027          0.019           0.064   \n",
       "con_n_periods               -0.001          0.537           0.024   \n",
       "cat_schweiz                 -0.029          0.051           0.176   \n",
       "cat_finance                 -0.025          0.023           0.053   \n",
       "cat_sport                    0.020         -0.049          -0.018   \n",
       "cat_wissen                   0.034         -0.002          -0.015   \n",
       "cat_ausland                 -0.019          0.017           0.076   \n",
       "\n",
       "                        con_num_words_trans     ...       num_comments  \\\n",
       "score                                 0.015     ...             -0.029   \n",
       "score_trans                           0.034     ...             -0.033   \n",
       "vup                                   0.020     ...             -0.036   \n",
       "vdo                                   0.020     ...             -0.028   \n",
       "weekday                               0.002     ...              0.018   \n",
       "hour                                 -0.051     ...              0.023   \n",
       "weekday_fl                           -0.004     ...              0.021   \n",
       "con_len_trans                         0.989     ...              0.054   \n",
       "text_len_trans                        0.059     ...              0.136   \n",
       "con_num_words_trans                   1.000     ...              0.052   \n",
       "user_n_comments                      -0.090     ...             -0.021   \n",
       "text_num_words_trans                  0.058     ...              0.129   \n",
       "num_comments                          0.052     ...              1.000   \n",
       "num_comments_trans                    0.070     ...              0.792   \n",
       "time_since_first_trans                0.050     ...              0.044   \n",
       "header_len_trans                      0.016     ...              0.058   \n",
       "con_n_periods                         0.530     ...              0.020   \n",
       "cat_schweiz                           0.047     ...              0.115   \n",
       "cat_finance                           0.021     ...              0.188   \n",
       "cat_sport                            -0.043     ...             -0.152   \n",
       "cat_wissen                           -0.003     ...              0.031   \n",
       "cat_ausland                           0.013     ...             -0.101   \n",
       "\n",
       "                        num_comments_trans  time_since_first_trans  \\\n",
       "score                               -0.015                  -0.175   \n",
       "score_trans                         -0.019                  -0.175   \n",
       "vup                                 -0.018                  -0.215   \n",
       "vdo                                 -0.012                  -0.172   \n",
       "weekday                              0.025                   0.094   \n",
       "hour                                 0.009                  -0.073   \n",
       "weekday_fl                           0.026                   0.085   \n",
       "con_len_trans                        0.071                   0.049   \n",
       "text_len_trans                       0.198                  -0.034   \n",
       "con_num_words_trans                  0.070                   0.050   \n",
       "user_n_comments                     -0.018                   0.028   \n",
       "text_num_words_trans                 0.188                  -0.036   \n",
       "num_comments                         0.792                   0.044   \n",
       "num_comments_trans                   1.000                   0.078   \n",
       "time_since_first_trans               0.078                   1.000   \n",
       "header_len_trans                     0.053                  -0.014   \n",
       "con_n_periods                        0.029                   0.030   \n",
       "cat_schweiz                          0.144                  -0.082   \n",
       "cat_finance                          0.194                   0.035   \n",
       "cat_sport                           -0.152                   0.003   \n",
       "cat_wissen                           0.018                   0.022   \n",
       "cat_ausland                         -0.068                   0.008   \n",
       "\n",
       "                        header_len_trans  con_n_periods  cat_schweiz  \\\n",
       "score                              0.020          0.005        0.037   \n",
       "score_trans                        0.017          0.020        0.019   \n",
       "vup                                0.022          0.004        0.045   \n",
       "vdo                                0.011         -0.003        0.034   \n",
       "weekday                           -0.030          0.001       -0.024   \n",
       "hour                               0.017         -0.018       -0.040   \n",
       "weekday_fl                        -0.027         -0.001       -0.029   \n",
       "con_len_trans                      0.019          0.537        0.051   \n",
       "text_len_trans                     0.064          0.024        0.176   \n",
       "con_num_words_trans                0.016          0.530        0.047   \n",
       "user_n_comments                    0.011         -0.028       -0.002   \n",
       "text_num_words_trans               0.069          0.025        0.178   \n",
       "num_comments                       0.058          0.020        0.115   \n",
       "num_comments_trans                 0.053          0.029        0.144   \n",
       "time_since_first_trans            -0.014          0.030       -0.082   \n",
       "header_len_trans                   1.000         -0.005        0.090   \n",
       "con_n_periods                     -0.005          1.000       -0.000   \n",
       "cat_schweiz                        0.090         -0.000        1.000   \n",
       "cat_finance                        0.021          0.020       -0.406   \n",
       "cat_sport                         -0.150         -0.015       -0.248   \n",
       "cat_wissen                         0.013          0.001       -0.247   \n",
       "cat_ausland                       -0.001          0.003       -0.229   \n",
       "\n",
       "                        cat_finance  cat_sport  cat_wissen  cat_ausland  \n",
       "score                        -0.033     -0.021      -0.010        0.006  \n",
       "score_trans                  -0.015     -0.034      -0.002       -0.003  \n",
       "vup                          -0.047     -0.016      -0.015        0.020  \n",
       "vdo                          -0.053      0.013      -0.019        0.050  \n",
       "weekday                      -0.023      0.018       0.029       -0.020  \n",
       "hour                         -0.018      0.013       0.043        0.008  \n",
       "weekday_fl                   -0.025      0.020       0.034       -0.019  \n",
       "con_len_trans                 0.023     -0.049      -0.002        0.017  \n",
       "text_len_trans                0.053     -0.018      -0.015        0.076  \n",
       "con_num_words_trans           0.021     -0.043      -0.003        0.013  \n",
       "user_n_comments               0.012     -0.023      -0.019        0.013  \n",
       "text_num_words_trans          0.039     -0.008      -0.017        0.067  \n",
       "num_comments                  0.188     -0.152       0.031       -0.101  \n",
       "num_comments_trans            0.194     -0.152       0.018       -0.068  \n",
       "time_since_first_trans        0.035      0.003       0.022        0.008  \n",
       "header_len_trans              0.021     -0.150       0.013       -0.001  \n",
       "con_n_periods                 0.020     -0.015       0.001        0.003  \n",
       "cat_schweiz                  -0.406     -0.248      -0.247       -0.229  \n",
       "cat_finance                   1.000     -0.124      -0.124       -0.115  \n",
       "cat_sport                    -0.124      1.000      -0.076       -0.070  \n",
       "cat_wissen                   -0.124     -0.076       1.000       -0.070  \n",
       "cat_ausland                  -0.115     -0.070      -0.070        1.000  \n",
       "\n",
       "[22 rows x 22 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# User based information\n",
    "temp = df_merge[['aut']].groupby('aut').size().reset_index()\n",
    "temp = temp.rename(columns = {0: 'user_n_comments'})\n",
    "#temp2 = pd.get_dummies(temp[(temp['user_n_comments']<100) & (temp['user_n_comments']>50)], prefix='user', columns=['aut'])\n",
    "#print(\"users with certain amount of comments: \", len(temp2))\n",
    "temp2 = pd.get_dummies(df_merge[['cId', 'weekday']], prefix='wd_', columns=['weekday'])\n",
    "# merge here first because of index\n",
    "df = df_merge.merge(temp2, left_index=True, right_index=True) \n",
    "df = df.merge(temp, on='aut')\n",
    "\n",
    "df['con_n_periods'] = df['con'].apply(lambda x: len(x.split('.')))\n",
    "\n",
    "def sgn(x):\n",
    "    if x == 0: return 0\n",
    "    else: return x/abs(x)\n",
    "# Removes left/right skew \n",
    "for col in ['weekday_fl', 'con_len', 'text_len', 'time_since_first', \n",
    "            'num_comments', 'user_n_comments', 'con_n_periods',\n",
    "            'score', 'contr', 'header_len', 'con_num_words', 'text_num_words']:\n",
    "    df[col + '_trans'] = df[col].apply(lambda x: sgn(x)*math.log(abs(x) + 1))\n",
    "\n",
    "# Memory optimization\n",
    "# Technical stuff, contributes nothing to analysis\n",
    "conv = df.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='unsigned')\n",
    "df[conv.columns] = conv\n",
    "    \n",
    "# I removed very weak correlations to \"score\" and \"contr\"\n",
    "cols = ['score', 'score_trans', 'vup', 'vdo', 'weekday', 'hour', 'weekday_fl',\n",
    "       'con_len_trans', 'text_len_trans', 'con_num_words_trans', 'user_n_comments',\n",
    "        'text_num_words_trans', 'num_comments', 'num_comments_trans',\n",
    "       'time_since_first_trans', 'header_len_trans', 'con_n_periods',\n",
    "        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "       ]#'user_Amina123', 'user_Chris', 'user_Dani']\n",
    "# Get pearson co-efficients\n",
    "df[cols].corr()\n",
    "\n",
    "#df.hist('user_score_mean_trans')\n",
    "\n",
    "#for c in cols:\n",
    "#    print(c)\n",
    "#    print(df[np.isnan(df[col])].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and split for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data:  50693\n"
     ]
    }
   ],
   "source": [
    "# https://de.wikipedia.org/wiki/Liste_der_h%C3%A4ufigsten_W%C3%B6rter_der_deutschen_Sprache\n",
    "stop_words = \"die, der, und, in, zu, den, das, nicht, von, sie, ist, des, sich, mit, dem, dass, er, es, ein, ich, auf, so, eine, auch, als, an, nach, wie, im, für\"\n",
    "stop_words += \"man, aber, aus, durch, wenn, nur, war, noch, werden, bei, hat, wir, was, wird, sein, einen, welche, sind, oder, zur, um, haben, einer, mir, über, ihm, diese, einem, ihr, uns\"\n",
    "stop_words += \"da, zum, kann, doch, vor, dieser, mich, ihn, du, hatte, seine, mehr, am, denn, nun, unter, sehr, selbst, schon, hier\"\n",
    "stop_words += \"bis, habe, ihre, dann, ihnen, seiner, alle, wieder, meine, Zeit, gegen, vom, ganz, einzelnen, wo, muss, ohne, eines, können, sei\"\n",
    "stop_words = stop_words.lower()\n",
    "stop_words = stop_words.split(', ')\n",
    "\n",
    "X = df.drop(['score', 'contr', 'vup', 'vdo'], axis=1)\n",
    "y = df['score']\n",
    "\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "print(\"total data: \", len(X))\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, ylim=None, n_jobs=1, train_sizes=np.linspace(.5, 1.0, 3)):\n",
    "    plt.figure()\n",
    "    plt.title('Learning curve')\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel('Training examples'); plt.ylabel('Score')\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=2, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, explained_variance_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Select a subset of data at a provided key.\n",
    "    key: hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    '''\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.key]\n",
    "    \n",
    "ngram_range = (1, 1)\n",
    "cat_cols = [col for col in list(df.columns) if col.startswith('wd_') or col.startswith('art_')]\n",
    "no_numbers = lambda x: re.sub(r'(\\d[\\d\\.])+', '', x.lower())\n",
    "model = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        [\n",
    "            # comment + user information\n",
    "            ('statistics', Pipeline([\n",
    "                ('selector', ItemSelector(\n",
    "                    key=['con_len_trans', 'num_comments', \n",
    "                         'time_since_first_trans',# 'con_num_words',\n",
    "                          'user_n_comments_trans', #'con_n_periods_trans',\n",
    "                        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "                         'cat_panorama', 'cat_community', 'cat_people', 'cat_digital'\n",
    "                   #     ])),\n",
    "                        ] + cat_cols)),\n",
    "                # Polynomialfeatures can help a little bit...\n",
    "                #('polynomialfeatures', PolynomialFeatures(degree=2)),\n",
    "              #  ('scaler', StandardScaler()),\n",
    "                ('reduce_dim', PCA(n_components=20)),\n",
    "            ])),\n",
    "\n",
    "            ('words_content', Pipeline([\n",
    "                ('selector', ItemSelector(key='con')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            ('words_title', Pipeline([\n",
    "                ('selector', ItemSelector(key='tit')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            # article information\n",
    "            ('words_header', Pipeline([\n",
    "                ('selector', ItemSelector(key='header')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            ('words_subheader', Pipeline([\n",
    "                ('selector', ItemSelector(key='sub')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            ('words_text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=0.2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "        ]\n",
    "    )),\n",
    "    #('reduce_dim', PCA(n_components=50)),#SelectKBest(f_regression)),\n",
    "    #('reduce_dim', TruncatedSVD(n_components=100)),\n",
    "    #('model', MLPRegressor(max_iter=50, hidden_layer_sizes=(100,)))\n",
    "    ('model', GradientBoostingRegressor(n_estimators=100, max_depth=3))\n",
    "])\n",
    "\n",
    "if False:\n",
    "    plot_learning_curve(model, X, y)\n",
    "    plt.show()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R^2: %1.3f\" % r2_score(y_test, y_pred))\n",
    "print(\"Explained var: {:3f}\".format(explained_variance_score(y_test, y_pred)))\n",
    "print(\"Mean absolute err: {:3f}\".format(mean_absolute_error(y_test, y_pred)))\n",
    "print(\"Mean squared err: {:3f}\".format(mean_squared_error(y_test, y_pred)))\n",
    "# Residual plot\n",
    "# time_since_first because it seems to have some influence (see feature importance below)\n",
    "X_res = X_test['time_since_first_trans']\n",
    "plt.scatter(X_res, y_test, color='black', label='test data')\n",
    "plt.scatter(X_res, y_pred, color='red', label='predicted')\n",
    "plt.xlabel('time_since_first_trans')\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "if False:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_grid = {\n",
    "        'model__hidden_layer_sizes':[(50,), (100,), (200,), (100, 50)]\n",
    "#        'motdel__alpha': np.logspace(-1, 4, 6),\n",
    "                 }\n",
    "    grid = GridSearchCV(model, param_grid, cv=3)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary so far\n",
    "The decision tree regressor depends a lot on max_depth parameter: Depending on it, the model over- or underfits. I reached the best R^2 score with max_depth=3, but model underfits.\n",
    "\n",
    "Neural networks work a bit better: R^2 = 0.39 (old try)  \n",
    "GBRF same: R^2 = 0.45  \n",
    "ngram range (1, 2): 0.37  ngram range (1, 1): 0.366   \n",
    "adding more stop words: gives a very small increase\n",
    "\n",
    "The models improve slightly with more data, will try bigger runs.\n",
    "\n",
    "How to extract more information? How to use user behaviour?\n",
    "\n",
    "TODO: better text feature extration. Save trained model, make a prediction function where it's easy to input data to try around with score prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try to find out which features are not important\n",
    "# It seems that the text features are rather important\n",
    "\n",
    "fi = model.named_steps['model'].feature_importances_\n",
    "print(len(['weekday', 'hour', 'con_len_trans', 'num_comments', \n",
    "                         'time_since_first_trans',# 'con_num_words',\n",
    "                          'user_n_comments_trans', #'con_n_periods_trans',\n",
    "                        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "                         'cat_panorama', 'cat_community', 'cat_people', 'cat_digital'\n",
    "                       ] + user_cols))\n",
    "print(len(fi[:-120]))\n",
    "print('content')\n",
    "print(fi[-120:-90])\n",
    "print('tit')\n",
    "print(fi[-90:-60])\n",
    "print('sub')\n",
    "print(fi[-60:-30])\n",
    "print('text')\n",
    "print(fi[-30:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
