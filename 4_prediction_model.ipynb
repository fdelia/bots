{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction of User Reactions\n",
    "We want to predict:\n",
    "* How to get high score/consensus (upvotes - downvotes)\n",
    "* How to be controversial (upvotes + downvotes)  \n",
    "in the comments.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Not necessary, but I like the ggplot style better\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df_art = pd.read_csv('articles_2017_08.csv')\n",
    "df_com = pd.read_csv('comments_2017_08.csv').head(20000) # crop because battery life, skews data\n",
    "# Make float better readable\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_dt_obj(time):\n",
    "    time = time.replace('am ', '')\n",
    "    # Make datetime object from string\n",
    "    return datetime.strptime(time, '%d.%m.%Y %H:%M')\n",
    "\n",
    "def time_since_epoch(time):\n",
    "    return (get_dt_obj(time)-datetime(1970,1,1)).total_seconds()\n",
    "\n",
    "def get_hour_of_day(time):\n",
    "    return get_dt_obj(time).hour\n",
    "\n",
    "def get_weekday(time):\n",
    "    return get_dt_obj(time).weekday()\n",
    "\n",
    "# Basically same as \"the hour of week\" or \"weekday_hourOfDay\"\n",
    "def get_weekday_float(time):\n",
    "    hour = float(get_hour_of_day(time))\n",
    "    weekday = get_weekday(time)\n",
    "    return float(weekday) + hour / 24\n",
    "\n",
    "def get_weekday_hour(time):\n",
    "    return '{}_{}'.format(get_weekday(time), get_hour_of_day(time))\n",
    "\n",
    "df_com['time_since_epoch'] = df_com['time'].apply(time_since_epoch)\n",
    "df_com['hour'] = df_com['time'].apply(get_hour_of_day)\n",
    "df_com['weekday'] = df_com['time'].apply(get_weekday) # 0 = Monday\n",
    "df_com['weekday_fl'] = df_com['time'].apply(get_weekday_float)\n",
    "df_com['weekday_hour'] = df_com['time'].apply(get_weekday_hour)\n",
    "df_com['is_answer'] = df_com['tit'].apply(lambda x: str(x).startswith('@'))\n",
    "df_com['con_len'] = df_com['con'].apply(lambda x: len(x))\n",
    "df_com['con_num_words'] = df_com['con'].apply(lambda x: len(x.split()))\n",
    "df_com['score'] = df_com['vup'] - df_com['vdo']\n",
    "df_com['contr'] = df_com['vup'] + df_com['vdo']\n",
    "\n",
    "df_com['tit'] = df_com['tit'].str.lower()\n",
    "df_com['con'] = df_com['con'].str.lower()\n",
    "\n",
    "def get_category(link):\n",
    "    t = link.split('/')\n",
    "    if len(t) <= 1:\n",
    "        return ''\n",
    "    else:\n",
    "        return t[1]\n",
    "\n",
    "df_art['cat'] = df_art['link'].apply(get_category)\n",
    "df_art['header_len'] = df_art['header'].apply(lambda x: len(x))\n",
    "df_art['text_len'] = df_art['text'].apply(lambda x: len(str(x)))\n",
    "df_art['text_num_words'] = df_art['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Left inner join\n",
    "df_merge = pd.merge(left=df_com, right=df_art, left_on='tId', right_on='tId')\n",
    "\n",
    "# is empty: (= all comments correspond to an article)\n",
    "#print(df_merge[pd.isnull(df_merge['link'])])\n",
    "#df_merge[df_merge['vup']>1000]\n",
    "\n",
    "# Get order of comments per article\n",
    "df_merge_art = df_merge.sort_values(['tId', 'time_since_epoch']).groupby('tId')\n",
    "# Get time since the first comment\n",
    "def get_time_since_first(group):\n",
    "    first = group.iloc[:1]['time_since_epoch']\n",
    "    group['time_since_first'] = group['time_since_epoch'].apply(lambda x: (x - first) / 3600)\n",
    "    # Remove those very late comments, after x hours\n",
    "    group = group[group['time_since_first'] < 36]\n",
    "    return group\n",
    "\n",
    "df_merge_art = df_merge_art.apply(get_time_since_first)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to remove skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_cat\n",
      "<0       6477\n",
      "small    7324\n",
      "big      6110\n",
      "dtype: int64\n",
      "score_trans\n",
      "Empty DataFrame\n",
      "Columns: [tId, cId, mob, vup, vdo, tit, aut, time, con, time_since_epoch, hour, weekday, weekday_fl, weekday_hour, is_answer, con_len, con_num_words, score, contr, article_id, updated, num_comments, link, header, sub, text, cat, header_len, text_len, text_num_words, time_since_first, score_cat, weekday_fl_trans, con_len_trans, text_len_trans, time_since_first_trans, score_trans, contr_trans, header_len_trans, con_num_words_trans, text_num_words_trans]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "contr\n",
      "Empty DataFrame\n",
      "Columns: [tId, cId, mob, vup, vdo, tit, aut, time, con, time_since_epoch, hour, weekday, weekday_fl, weekday_hour, is_answer, con_len, con_num_words, score, contr, article_id, updated, num_comments, link, header, sub, text, cat, header_len, text_len, text_num_words, time_since_first, score_cat, weekday_fl_trans, con_len_trans, text_len_trans, time_since_first_trans, score_trans, contr_trans, header_len_trans, con_num_words_trans, text_num_words_trans]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "weekday\n",
      "Empty DataFrame\n",
      "Columns: [tId, cId, mob, vup, vdo, tit, aut, time, con, time_since_epoch, hour, weekday, weekday_fl, weekday_hour, is_answer, con_len, con_num_words, score, contr, article_id, updated, num_comments, link, header, sub, text, cat, header_len, text_len, text_num_words, time_since_first, score_cat, weekday_fl_trans, con_len_trans, text_len_trans, time_since_first_trans, score_trans, contr_trans, header_len_trans, con_num_words_trans, text_num_words_trans]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "hour\n",
      "Empty DataFrame\n",
      "Columns: [tId, cId, mob, vup, vdo, tit, aut, time, con, time_since_epoch, hour, weekday, weekday_fl, weekday_hour, is_answer, con_len, con_num_words, score, contr, article_id, updated, num_comments, link, header, sub, text, cat, header_len, text_len, text_num_words, time_since_first, score_cat, weekday_fl_trans, con_len_trans, text_len_trans, time_since_first_trans, score_trans, contr_trans, header_len_trans, con_num_words_trans, text_num_words_trans]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "weekday_fl\n",
      "Empty DataFrame\n",
      "Columns: [tId, cId, mob, vup, vdo, tit, aut, time, con, time_since_epoch, hour, weekday, weekday_fl, weekday_hour, is_answer, con_len, con_num_words, score, contr, article_id, updated, num_comments, link, header, sub, text, cat, header_len, text_len, text_num_words, time_since_first, score_cat, weekday_fl_trans, con_len_trans, text_len_trans, time_since_first_trans, score_trans, contr_trans, header_len_trans, con_num_words_trans, text_num_words_trans]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "con_len_trans\n",
      "Empty DataFrame\n",
      "Columns: [tId, cId, mob, vup, vdo, tit, aut, time, con, time_since_epoch, hour, weekday, weekday_fl, weekday_hour, is_answer, con_len, con_num_words, score, contr, article_id, updated, num_comments, link, header, sub, text, cat, header_len, text_len, text_num_words, time_since_first, score_cat, weekday_fl_trans, con_len_trans, text_len_trans, time_since_first_trans, score_trans, contr_trans, header_len_trans, con_num_words_trans, text_num_words_trans]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "text_len_trans\n",
      "Empty DataFrame\n",
      "Columns: [tId, cId, mob, vup, vdo, tit, aut, time, con, time_since_epoch, hour, weekday, weekday_fl, weekday_hour, is_answer, con_len, con_num_words, score, contr, article_id, updated, num_comments, link, header, sub, text, cat, header_len, text_len, text_num_words, time_since_first, score_cat, weekday_fl_trans, con_len_trans, text_len_trans, time_since_first_trans, score_trans, contr_trans, header_len_trans, con_num_words_trans, text_num_words_trans]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "con_num_words_trans\n",
      "Empty DataFrame\n",
      "Columns: [tId, cId, mob, vup, vdo, tit, aut, time, con, time_since_epoch, hour, weekday, weekday_fl, weekday_hour, is_answer, con_len, con_num_words, score, contr, article_id, updated, num_comments, link, header, sub, text, cat, header_len, text_len, text_num_words, time_since_first, score_cat, weekday_fl_trans, con_len_trans, text_len_trans, time_since_first_trans, score_trans, contr_trans, header_len_trans, con_num_words_trans, text_num_words_trans]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "text_num_words_trans\n",
      "Empty DataFrame\n",
      "Columns: [tId, cId, mob, vup, vdo, tit, aut, time, con, time_since_epoch, hour, weekday, weekday_fl, weekday_hour, is_answer, con_len, con_num_words, score, contr, article_id, updated, num_comments, link, header, sub, text, cat, header_len, text_len, text_num_words, time_since_first, score_cat, weekday_fl_trans, con_len_trans, text_len_trans, time_since_first_trans, score_trans, contr_trans, header_len_trans, con_num_words_trans, text_num_words_trans]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "time_since_first_trans\n",
      "Empty DataFrame\n",
      "Columns: [tId, cId, mob, vup, vdo, tit, aut, time, con, time_since_epoch, hour, weekday, weekday_fl, weekday_hour, is_answer, con_len, con_num_words, score, contr, article_id, updated, num_comments, link, header, sub, text, cat, header_len, text_len, text_num_words, time_since_first, score_cat, weekday_fl_trans, con_len_trans, text_len_trans, time_since_first_trans, score_trans, contr_trans, header_len_trans, con_num_words_trans, text_num_words_trans]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "header_len_trans\n",
      "Empty DataFrame\n",
      "Columns: [tId, cId, mob, vup, vdo, tit, aut, time, con, time_since_epoch, hour, weekday, weekday_fl, weekday_hour, is_answer, con_len, con_num_words, score, contr, article_id, updated, num_comments, link, header, sub, text, cat, header_len, text_len, text_num_words, time_since_first, score_cat, weekday_fl_trans, con_len_trans, text_len_trans, time_since_first_trans, score_trans, contr_trans, header_len_trans, con_num_words_trans, text_num_words_trans]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def show_hist(col):\n",
    "    df_merge_art['bins'] = pd.cut(df_merge_art[col], 100)\n",
    "    h = df_merge_art.groupby(['bins']).size().plot.bar(title=col)\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    \n",
    "# This categories are random\n",
    "bins = [-np.inf, 2, 25, np.inf]\n",
    "labels = ['<0', 'small', 'big']\n",
    "df_merge_art['score_cat'] = pd.cut(df_merge_art['score'], bins, labels=labels)\n",
    "print(df_merge_art.groupby('score_cat').size())\n",
    "\n",
    "#df = df[(df['score']<2) | (df['score']>=25)]\n",
    "    \n",
    "def sgn(x):\n",
    "    if x == 0: return 0\n",
    "    else: return x/abs(x)\n",
    "# Removes left/right skew \n",
    "transformation = lambda x: sgn(x)*math.log(abs(x) + 1)\n",
    "for col in ['weekday_fl', 'con_len', 'text_len', 'time_since_first', \n",
    "            'score', 'contr', 'header_len', 'con_num_words', 'text_num_words']:\n",
    "    #min_val = df_merge_art[col].min()\n",
    "    df[col + '_trans'] = df_merge_art[col].apply(transformation)\n",
    "    #show_hist(col + '_trans')\n",
    "\n",
    "# I removed very weak correlations to \"score\" and \"contr\"\n",
    "cols = ['score_trans', 'contr', 'weekday', 'hour', 'weekday_fl',\n",
    "       'con_len_trans', 'text_len_trans', 'con_num_words_trans', \n",
    "        'text_num_words_trans',\n",
    "       'time_since_first_trans', 'header_len_trans']\n",
    "# Get pearson co-efficients\n",
    "df[cols].corr()\n",
    "\n",
    "for c in cols:\n",
    "    print(c)\n",
    "    print(df[np.isnan(df[col])].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and split for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://de.wikipedia.org/wiki/Liste_der_h%C3%A4ufigsten_W%C3%B6rter_der_deutschen_Sprache\n",
    "stop_words = \"die, der, und, in, zu, den, das, nicht, von, sie, ist, des, sich, mit, dem, dass, er, es, ein, ich, auf, so, eine, auch, als, an, nach, wie, im, für\"\n",
    "#stop_words += \"man, aber, aus, durch, wenn, nur, war, noch, werden, bei, hat, wir, was, wird, sein, einen, welche, sind, oder, zur, um, haben, einer, mir, über, ihm, diese, einem, ihr, uns\"\n",
    "#stop_words += \"da, zum, kann, doch, vor, dieser, mich, ihn, du, hatte, seine, mehr, am, denn, nun, unter, sehr, selbst, schon, hier\"\n",
    "#stop_words += \"bis, habe, ihre, dann, ihnen, seiner, alle, wieder, meine, Zeit, gegen, vom, ganz, einzelnen, wo, muss, ohne, eines, können, sei\"\n",
    "stop_words = stop_words.lower()\n",
    "stop_words = stop_words.split(', ')\n",
    "\n",
    "def classify(val):\n",
    "    if val > 10: return 1\n",
    "    return 0\n",
    "\n",
    "X = df.drop(['score', 'contr'], axis=1)\n",
    "\n",
    "#y = df_merge_art['score'].apply(classify)\n",
    "y = df['score']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "#from sklearn.feature_selection import SelectKBest, SelectPercentile, f_regression\n",
    "#from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Select a subset of data at a provided key.\n",
    "    key: hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    '''\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.key]\n",
    "    \n",
    "# TODO onehotencoder for \"cat\"\n",
    "no_numbers = lambda x: re.sub(r'(\\d[\\d\\.])+', '', x.lower())\n",
    "model = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=\n",
    "        [\n",
    "            ('statistics', Pipeline([\n",
    "                ('selector', ItemSelector(\n",
    "                    key=['weekday', 'hour', 'con_len_trans', \n",
    "                         'time_since_first_trans'])),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('polynomialfeatures', PolynomialFeatures(degree=5))\n",
    "            ]))#,\n",
    "            \n",
    "            # Course of dimensionality!\n",
    "         #   ('words', Pipeline([\n",
    "          #      ('selector', ItemSelector(key='con')),\n",
    "            #    ('vect', CountVectorizer(preprocessor=no_numbers, ngram_range=(1, 1), stop_words=stop_words))\n",
    "           #     ('sent_model', sent_model)\n",
    "           # ]))\n",
    "        ],\n",
    "        \n",
    "        transformer_weights={\n",
    "            'statistics': .8,\n",
    "            'words': .2\n",
    "        }\n",
    "    )),\n",
    "    \n",
    "    # Params not optimized yet\n",
    "    ('model', Ridge())\n",
    "    #('model', SVC(kernel='linear', n_jobs=-1))\n",
    "])\n",
    "\n",
    "#model = make_pipeline(ItemSelector(\n",
    "#        key=['weekday', 'hour', 'time_since_first', 'con_len']),\n",
    "#        StandardScaler(), PolynomialFeatures(degree=5), Ridge(alpha=100, max_iter=10, solver='cholesky'))\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R^2: %1.3f\\n\" % r2_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "if False:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_grid = {'polynomialfeatures__degree': [2, 3, 4, 5],\n",
    "        'ridge__alpha': np.logspace(-1, 4, 6), 'ridge__max_iter': [10, 50, 100], 'ridge__solver': ['svd', 'cholesky', 'lsqr']}\n",
    "    grid = GridSearchCV(model, param_grid, cv=3)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best estimator:\\n{}\".format(grid.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
