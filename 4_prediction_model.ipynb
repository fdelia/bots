{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction of User Reactions\n",
    "We want to predict:\n",
    "* How to get high score/consensus (upvotes - downvotes)\n",
    "* How to be controversial (upvotes + downvotes)  \n",
    "in the comments.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Not necessary, but I like the ggplot style better\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df_art = pd.read_csv('articles_2017_09.csv')\n",
    "df_com = pd.read_csv('comments_2017_09.csv').sample(150000) # crop because battery life\n",
    "# Make float better readable\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tId</th>\n",
       "      <th>cId</th>\n",
       "      <th>mob</th>\n",
       "      <th>vup</th>\n",
       "      <th>vdo</th>\n",
       "      <th>tit</th>\n",
       "      <th>aut</th>\n",
       "      <th>time</th>\n",
       "      <th>con</th>\n",
       "      <th>time_since_epoch</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_people</th>\n",
       "      <th>cat_play</th>\n",
       "      <th>cat_playview</th>\n",
       "      <th>cat_schweiz</th>\n",
       "      <th>cat_sport</th>\n",
       "      <th>cat_wissen</th>\n",
       "      <th>header_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_num_words</th>\n",
       "      <th>time_since_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117896</th>\n",
       "      <td>10003016</td>\n",
       "      <td>1_13</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>49</td>\n",
       "      <td>mein erster gedanke</td>\n",
       "      <td>H. M.</td>\n",
       "      <td>am 08.08.2017 07:53</td>\n",
       "      <td>ich denke es auch nicht gerade förderlich wen...</td>\n",
       "      <td>1502178780.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>3429</td>\n",
       "      <td>509</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117895</th>\n",
       "      <td>10003016</td>\n",
       "      <td>15_15</td>\n",
       "      <td>1</td>\n",
       "      <td>254</td>\n",
       "      <td>15</td>\n",
       "      <td>abgründe</td>\n",
       "      <td>Justitias Hand</td>\n",
       "      <td>am 08.08.2017 07:54</td>\n",
       "      <td>ich war schon beruflich im darknet unterwegs....</td>\n",
       "      <td>1502178840.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>3429</td>\n",
       "      <td>509</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117893</th>\n",
       "      <td>10003016</td>\n",
       "      <td>43_43</td>\n",
       "      <td>1</td>\n",
       "      <td>248</td>\n",
       "      <td>26</td>\n",
       "      <td>beim ersten bericht mit dem</td>\n",
       "      <td>kein Name</td>\n",
       "      <td>am 08.08.2017 08:17</td>\n",
       "      <td>entführtem model, war mein erster gedanke; fa...</td>\n",
       "      <td>1502180220.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>3429</td>\n",
       "      <td>509</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             tId    cId  mob  vup  vdo                          tit  \\\n",
       "117896  10003016   1_13    1  190   49          mein erster gedanke   \n",
       "117895  10003016  15_15    1  254   15                     abgründe   \n",
       "117893  10003016  43_43    1  248   26  beim ersten bericht mit dem   \n",
       "\n",
       "                   aut                 time  \\\n",
       "117896           H. M.  am 08.08.2017 07:53   \n",
       "117895  Justitias Hand  am 08.08.2017 07:54   \n",
       "117893       kein Name  am 08.08.2017 08:17   \n",
       "\n",
       "                                                      con  time_since_epoch  \\\n",
       "117896   ich denke es auch nicht gerade förderlich wen...    1502178780.000   \n",
       "117895   ich war schon beruflich im darknet unterwegs....    1502178840.000   \n",
       "117893   entführtem model, war mein erster gedanke; fa...    1502180220.000   \n",
       "\n",
       "              ...         cat_people  cat_play  cat_playview  cat_schweiz  \\\n",
       "117896        ...                  0         0             0            0   \n",
       "117895        ...                  0         0             0            0   \n",
       "117893        ...                  0         0             0            0   \n",
       "\n",
       "        cat_sport  cat_wissen  header_len  text_len  text_num_words  \\\n",
       "117896          0           0          41      3429             509   \n",
       "117895          0           0          41      3429             509   \n",
       "117893          0           0          41      3429             509   \n",
       "\n",
       "        time_since_first  \n",
       "117896             0.000  \n",
       "117895             0.017  \n",
       "117893             0.400  \n",
       "\n",
       "[3 rows x 46 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_dt_obj(time):\n",
    "    time = time.replace('am ', '')\n",
    "    # Make datetime object from string\n",
    "    return datetime.strptime(time, '%d.%m.%Y %H:%M')\n",
    "\n",
    "def time_since_epoch(time):\n",
    "    return (get_dt_obj(time)-datetime(1970,1,1)).total_seconds()\n",
    "\n",
    "def get_hour_of_day(time):\n",
    "    return get_dt_obj(time).hour\n",
    "\n",
    "def get_weekday(time):\n",
    "    return get_dt_obj(time).weekday()\n",
    "\n",
    "# Basically same as \"the hour of week\" or \"weekday_hourOfDay\"\n",
    "def get_weekday_float(time):\n",
    "    hour = float(get_hour_of_day(time))\n",
    "    weekday = get_weekday(time)\n",
    "    return float(weekday) + hour / 24\n",
    "\n",
    "def get_weekday_hour(time):\n",
    "    return '{}_{}'.format(get_weekday(time), get_hour_of_day(time))\n",
    "\n",
    "df_com['time_since_epoch'] = df_com['time'].apply(time_since_epoch)\n",
    "df_com['hour'] = df_com['time'].apply(get_hour_of_day)\n",
    "df_com['weekday'] = df_com['time'].apply(get_weekday) # 0 = Monday\n",
    "df_com['weekday_fl'] = df_com['time'].apply(get_weekday_float)\n",
    "#df_com['weekday_hour'] = df_com['time'].apply(get_weekday_hour)\n",
    "df_com['is_answer'] = df_com['tit'].apply(lambda x: str(x).startswith('@'))\n",
    "df_com['con_len'] = df_com['con'].apply(lambda x: len(x))\n",
    "df_com['con_num_words'] = df_com['con'].apply(lambda x: len(x.split()))\n",
    "df_com['score'] = df_com['vup'] - df_com['vdo']\n",
    "df_com['contr'] = df_com['vup'] + df_com['vdo']\n",
    "\n",
    "df_com['tit'] = df_com['tit'].str.lower()\n",
    "df_com['con'] = df_com['con'].str.lower()\n",
    "\n",
    "def get_category(link):\n",
    "    t = link.split('/')\n",
    "    if len(t) <= 1:\n",
    "        return ''\n",
    "    else:\n",
    "        return t[1]\n",
    "\n",
    "df_art['cat'] = df_art['link'].apply(get_category)\n",
    "df_art['cat_copy'] = df_art['cat']\n",
    "df_art = pd.get_dummies(df_art, columns=['cat'])\n",
    "\n",
    "df_art['header_len'] = df_art['header'].apply(lambda x: len(x))\n",
    "df_art['text_len'] = df_art['text'].apply(lambda x: len(str(x)))\n",
    "df_art['text_num_words'] = df_art['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Left inner join\n",
    "df_merge = pd.merge(left=df_com, right=df_art, left_on='tId', right_on='tId')\n",
    "\n",
    "# Remove rows with missing values\n",
    "# use .count() to check for missing values\n",
    "df_merge.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# Get order of comments per article\n",
    "df_merge = df_merge.sort_values(['tId', 'time_since_epoch']).groupby('tId')\n",
    "\n",
    "# Get time since the first comment\n",
    "def get_time_since_first(group):\n",
    "    first = group.iloc[:1]['time_since_epoch']\n",
    "    group['time_since_first'] = group['time_since_epoch'].apply(lambda x: (x - first) / 3600)\n",
    "    # Remove those very late comments, after x hours\n",
    "    #group = group[group['time_since_first'] < 36]\n",
    "    return group\n",
    "\n",
    "# Creating \"copy\" to make next cell independent\n",
    "df_merge = df_merge.apply(get_time_since_first)\n",
    "df_merge.head(3)\n",
    "#df[df.index.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to remove skew\n",
    "__Rescaling__: Add or subtract a constant and then multiply or divide by a constant.  \n",
    "__Normalizing__: Dividing by a norm of the vector, e.g. make Euclidean length equal to one. Sometimes make all elements lie in [0, 1].  \n",
    "__Standardizing__: Subtracting a measure of location and dividing by a measure of scale. Eg. subtract the mean and divide by the std, thereby obtaining a standard normal distribution.\n",
    "\n",
    "These terms are sometimes used interchangeably.\n",
    "\n",
    "It's usually better to have the input values centered around zero, unless the output activation function has a range of [0, 1] (neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#temp = df[['aut', 'score']].groupby('aut').size().reset_index()\n",
    "#temp = temp.rename(columns = {0: 'user_n_comments'})\n",
    "#temp2 = pd.get_dummies(temp[temp['user_n_comments']>60], prefix='user_', columns=['aut'])\n",
    "#temp = temp.merge(temp2)\n",
    "#temp.head(2)\n",
    "#df.head(1)\n",
    "#user_cols = [col for col in list(df.columns) if col.startswith('user_')]\n",
    "#print(user_cols)\n",
    "#df_com[df_com['aut'] == 'Chris'][['aut', 'score']]\n",
    "#list(df.columns)\n",
    "#df.groupby('aut')[['aut', 'score']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users with lots of comments:  117\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>score_trans</th>\n",
       "      <th>vup</th>\n",
       "      <th>vdo</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday_fl</th>\n",
       "      <th>con_len_trans</th>\n",
       "      <th>text_len_trans</th>\n",
       "      <th>con_num_words_trans</th>\n",
       "      <th>...</th>\n",
       "      <th>header_len_trans</th>\n",
       "      <th>con_n_periods</th>\n",
       "      <th>cat_schweiz</th>\n",
       "      <th>cat_finance</th>\n",
       "      <th>cat_sport</th>\n",
       "      <th>cat_wissen</th>\n",
       "      <th>cat_ausland</th>\n",
       "      <th>user_Amina123</th>\n",
       "      <th>user_Chris</th>\n",
       "      <th>user_Dani</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_trans</th>\n",
       "      <td>0.481</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vup</th>\n",
       "      <td>0.964</td>\n",
       "      <td>0.372</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vdo</th>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.376</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_fl</th>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.145</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_len_trans</th>\n",
       "      <td>0.026</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len_trans</th>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.066</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_num_words_trans</th>\n",
       "      <td>0.024</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.065</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_n_comments</th>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_num_words_trans</th>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_comments</th>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_comments_trans</th>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_since_first_trans</th>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>header_len_trans</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.013</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_n_periods</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_schweiz</th>\n",
       "      <td>0.027</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_finance</th>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_sport</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_wissen</th>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_ausland</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_Amina123</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_Chris</th>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_Dani</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        score  score_trans    vup    vdo  weekday   hour  \\\n",
       "score                   1.000        0.481  0.964  0.114    0.003 -0.004   \n",
       "score_trans             0.481        1.000  0.372 -0.285   -0.002 -0.003   \n",
       "vup                     0.964        0.372  1.000  0.376    0.005 -0.002   \n",
       "vdo                     0.114       -0.285  0.376  1.000    0.008  0.008   \n",
       "weekday                 0.003       -0.002  0.005  0.008    1.000  0.027   \n",
       "hour                   -0.004       -0.003 -0.002  0.008    0.027  1.000   \n",
       "weekday_fl              0.002       -0.002  0.005  0.009    0.993  0.145   \n",
       "con_len_trans           0.026        0.058  0.033  0.032   -0.006 -0.053   \n",
       "text_len_trans          0.012       -0.001  0.016  0.018   -0.031 -0.030   \n",
       "con_num_words_trans     0.024        0.054  0.031  0.032   -0.008 -0.051   \n",
       "user_n_comments        -0.036       -0.070 -0.039 -0.018    0.018  0.090   \n",
       "text_num_words_trans    0.013        0.000  0.017  0.018   -0.039 -0.033   \n",
       "num_comments           -0.029       -0.045 -0.034 -0.026    0.038  0.024   \n",
       "num_comments_trans     -0.015       -0.029 -0.018 -0.016    0.046  0.014   \n",
       "time_since_first_trans -0.189       -0.190 -0.227 -0.192    0.096 -0.063   \n",
       "header_len_trans        0.019        0.007  0.020  0.008   -0.038  0.015   \n",
       "con_n_periods           0.025        0.024  0.029  0.022   -0.006 -0.013   \n",
       "cat_schweiz             0.027        0.011  0.033  0.031   -0.029 -0.031   \n",
       "cat_finance            -0.037       -0.038 -0.047 -0.048   -0.018 -0.013   \n",
       "cat_sport              -0.006       -0.001 -0.004  0.005    0.022  0.012   \n",
       "cat_wissen             -0.013        0.000 -0.018 -0.024    0.022  0.039   \n",
       "cat_ausland             0.012        0.011  0.026  0.054   -0.019 -0.003   \n",
       "user_Amina123           0.020        0.029  0.017 -0.007   -0.019  0.013   \n",
       "user_Chris             -0.004       -0.008 -0.002  0.007   -0.022 -0.015   \n",
       "user_Dani              -0.012       -0.010 -0.009  0.007    0.002 -0.012   \n",
       "\n",
       "                        weekday_fl  con_len_trans  text_len_trans  \\\n",
       "score                        0.002          0.026           0.012   \n",
       "score_trans                 -0.002          0.058          -0.001   \n",
       "vup                          0.005          0.033           0.016   \n",
       "vdo                          0.009          0.032           0.018   \n",
       "weekday                      0.993         -0.006          -0.031   \n",
       "hour                         0.145         -0.053          -0.030   \n",
       "weekday_fl                   1.000         -0.012          -0.034   \n",
       "con_len_trans               -0.012          1.000           0.066   \n",
       "text_len_trans              -0.034          0.066           1.000   \n",
       "con_num_words_trans         -0.014          0.989           0.065   \n",
       "user_n_comments              0.028         -0.127          -0.009   \n",
       "text_num_words_trans        -0.042          0.062           0.994   \n",
       "num_comments                 0.040          0.071           0.146   \n",
       "num_comments_trans           0.047          0.103           0.198   \n",
       "time_since_first_trans       0.088          0.043          -0.031   \n",
       "header_len_trans            -0.036          0.017           0.066   \n",
       "con_n_periods               -0.008          0.562           0.025   \n",
       "cat_schweiz                 -0.033          0.026           0.173   \n",
       "cat_finance                 -0.020          0.044           0.054   \n",
       "cat_sport                    0.023         -0.047          -0.015   \n",
       "cat_wissen                   0.026         -0.002          -0.022   \n",
       "cat_ausland                 -0.019          0.063           0.087   \n",
       "user_Amina123               -0.017         -0.026          -0.023   \n",
       "user_Chris                  -0.024          0.019           0.002   \n",
       "user_Dani                    0.000          0.013           0.006   \n",
       "\n",
       "                        con_num_words_trans    ...      header_len_trans  \\\n",
       "score                                 0.024    ...                 0.019   \n",
       "score_trans                           0.054    ...                 0.007   \n",
       "vup                                   0.031    ...                 0.020   \n",
       "vdo                                   0.032    ...                 0.008   \n",
       "weekday                              -0.008    ...                -0.038   \n",
       "hour                                 -0.051    ...                 0.015   \n",
       "weekday_fl                           -0.014    ...                -0.036   \n",
       "con_len_trans                         0.989    ...                 0.017   \n",
       "text_len_trans                        0.065    ...                 0.066   \n",
       "con_num_words_trans                   1.000    ...                 0.013   \n",
       "user_n_comments                      -0.129    ...                -0.006   \n",
       "text_num_words_trans                  0.062    ...                 0.073   \n",
       "num_comments                          0.069    ...                 0.049   \n",
       "num_comments_trans                    0.102    ...                 0.053   \n",
       "time_since_first_trans                0.043    ...                 0.005   \n",
       "header_len_trans                      0.013    ...                 1.000   \n",
       "con_n_periods                         0.558    ...                 0.014   \n",
       "cat_schweiz                           0.025    ...                 0.092   \n",
       "cat_finance                           0.043    ...                -0.001   \n",
       "cat_sport                            -0.039    ...                -0.133   \n",
       "cat_wissen                           -0.004    ...                 0.020   \n",
       "cat_ausland                           0.057    ...                -0.008   \n",
       "user_Amina123                        -0.039    ...                 0.002   \n",
       "user_Chris                            0.019    ...                -0.005   \n",
       "user_Dani                             0.014    ...                -0.003   \n",
       "\n",
       "                        con_n_periods  cat_schweiz  cat_finance  cat_sport  \\\n",
       "score                           0.025        0.027       -0.037     -0.006   \n",
       "score_trans                     0.024        0.011       -0.038     -0.001   \n",
       "vup                             0.029        0.033       -0.047     -0.004   \n",
       "vdo                             0.022        0.031       -0.048      0.005   \n",
       "weekday                        -0.006       -0.029       -0.018      0.022   \n",
       "hour                           -0.013       -0.031       -0.013      0.012   \n",
       "weekday_fl                     -0.008       -0.033       -0.020      0.023   \n",
       "con_len_trans                   0.562        0.026        0.044     -0.047   \n",
       "text_len_trans                  0.025        0.173        0.054     -0.015   \n",
       "con_num_words_trans             0.558        0.025        0.043     -0.039   \n",
       "user_n_comments                -0.008       -0.016        0.007     -0.020   \n",
       "text_num_words_trans            0.024        0.176        0.041     -0.007   \n",
       "num_comments                    0.025        0.140        0.156     -0.142   \n",
       "num_comments_trans              0.043        0.156        0.175     -0.147   \n",
       "time_since_first_trans          0.005       -0.080        0.059     -0.014   \n",
       "header_len_trans                0.014        0.092       -0.001     -0.133   \n",
       "con_n_periods                   1.000       -0.007        0.038     -0.032   \n",
       "cat_schweiz                    -0.007        1.000       -0.423     -0.228   \n",
       "cat_finance                     0.038       -0.423        1.000     -0.116   \n",
       "cat_sport                      -0.032       -0.228       -0.116      1.000   \n",
       "cat_wissen                      0.014       -0.234       -0.119     -0.064   \n",
       "cat_ausland                     0.022       -0.259       -0.132     -0.071   \n",
       "user_Amina123                   0.148       -0.017        0.003      0.000   \n",
       "user_Chris                      0.017        0.015       -0.009     -0.003   \n",
       "user_Dani                       0.018       -0.016        0.015     -0.001   \n",
       "\n",
       "                        cat_wissen  cat_ausland  user_Amina123  user_Chris  \\\n",
       "score                       -0.013        0.012          0.020      -0.004   \n",
       "score_trans                  0.000        0.011          0.029      -0.008   \n",
       "vup                         -0.018        0.026          0.017      -0.002   \n",
       "vdo                         -0.024        0.054         -0.007       0.007   \n",
       "weekday                      0.022       -0.019         -0.019      -0.022   \n",
       "hour                         0.039       -0.003          0.013      -0.015   \n",
       "weekday_fl                   0.026       -0.019         -0.017      -0.024   \n",
       "con_len_trans               -0.002        0.063         -0.026       0.019   \n",
       "text_len_trans              -0.022        0.087         -0.023       0.002   \n",
       "con_num_words_trans         -0.004        0.057         -0.039       0.019   \n",
       "user_n_comments             -0.003       -0.023          0.091       0.063   \n",
       "text_num_words_trans        -0.024        0.079         -0.023       0.002   \n",
       "num_comments                 0.002       -0.112         -0.035       0.003   \n",
       "num_comments_trans          -0.005       -0.083         -0.041       0.001   \n",
       "time_since_first_trans      -0.003        0.008         -0.080      -0.002   \n",
       "header_len_trans             0.020       -0.008          0.002      -0.005   \n",
       "con_n_periods                0.014        0.022          0.148       0.017   \n",
       "cat_schweiz                 -0.234       -0.259         -0.017       0.015   \n",
       "cat_finance                 -0.119       -0.132          0.003      -0.009   \n",
       "cat_sport                   -0.064       -0.071          0.000      -0.003   \n",
       "cat_wissen                   1.000       -0.073          0.012       0.008   \n",
       "cat_ausland                 -0.073        1.000         -0.002      -0.005   \n",
       "user_Amina123                0.012       -0.002          1.000      -0.011   \n",
       "user_Chris                   0.008       -0.005         -0.011       1.000   \n",
       "user_Dani                    0.011       -0.008         -0.011      -0.011   \n",
       "\n",
       "                        user_Dani  \n",
       "score                      -0.012  \n",
       "score_trans                -0.010  \n",
       "vup                        -0.009  \n",
       "vdo                         0.007  \n",
       "weekday                     0.002  \n",
       "hour                       -0.012  \n",
       "weekday_fl                  0.000  \n",
       "con_len_trans               0.013  \n",
       "text_len_trans              0.006  \n",
       "con_num_words_trans         0.014  \n",
       "user_n_comments             0.065  \n",
       "text_num_words_trans        0.005  \n",
       "num_comments               -0.001  \n",
       "num_comments_trans          0.000  \n",
       "time_since_first_trans      0.008  \n",
       "header_len_trans           -0.003  \n",
       "con_n_periods               0.018  \n",
       "cat_schweiz                -0.016  \n",
       "cat_finance                 0.015  \n",
       "cat_sport                  -0.001  \n",
       "cat_wissen                  0.011  \n",
       "cat_ausland                -0.008  \n",
       "user_Amina123              -0.011  \n",
       "user_Chris                 -0.011  \n",
       "user_Dani                   1.000  \n",
       "\n",
       "[25 rows x 25 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# User based information\n",
    "#temp = df_merge[['aut', 'score']].groupby('aut').mean() # <- this is cheating!\n",
    "#temp = temp.rename(columns = {'score': 'user_score_mean'}).reset_index()\n",
    "#df = df_merge.merge(temp, on='aut')\n",
    "temp = df_merge[['aut']].groupby('aut').size().reset_index()\n",
    "temp = temp.rename(columns = {0: 'user_n_comments'})\n",
    "temp2 = pd.get_dummies(temp[temp['user_n_comments']>100], prefix='user', columns=['aut'])\n",
    "print(\"users with lots of comments: \", len(temp2))\n",
    "temp = temp.merge(temp2).reset_index()\n",
    "df = df_merge.merge(temp, on='aut')\n",
    "\n",
    "df['con_n_periods'] = df['con'].apply(lambda x: len(x.split('.')))\n",
    "\n",
    "def sgn(x):\n",
    "    if x == 0: return 0\n",
    "    else: return x/abs(x)\n",
    "# Removes left/right skew \n",
    "for col in ['weekday_fl', 'con_len', 'text_len', 'time_since_first', \n",
    "            'num_comments', 'user_n_comments', 'con_n_periods',\n",
    "            'score', 'contr', 'header_len', 'con_num_words', 'text_num_words']:\n",
    "    df[col + '_trans'] = df[col].apply(lambda x: sgn(x)*math.log(abs(x) + 1))\n",
    "\n",
    "# Memory optimization\n",
    "# Technical stuff, contributes nothing to analysis\n",
    "conv = df.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='unsigned')\n",
    "df[conv.columns] = conv\n",
    "    \n",
    "# I removed very weak correlations to \"score\" and \"contr\"\n",
    "cols = ['score', 'score_trans', 'vup', 'vdo', 'weekday', 'hour', 'weekday_fl',\n",
    "       'con_len_trans', 'text_len_trans', 'con_num_words_trans', 'user_n_comments',\n",
    "        'text_num_words_trans', 'num_comments', 'num_comments_trans',\n",
    "       'time_since_first_trans', 'header_len_trans', 'con_n_periods',\n",
    "        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "       'user_Amina123', 'user_Chris', 'user_Dani']\n",
    "# Get pearson co-efficients\n",
    "df[cols].corr()\n",
    "\n",
    "#df.hist('user_score_mean_trans')\n",
    "\n",
    "#for c in cols:\n",
    "#    print(c)\n",
    "#    print(df[np.isnan(df[col])].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and split for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data:  32562\n"
     ]
    }
   ],
   "source": [
    "# https://de.wikipedia.org/wiki/Liste_der_h%C3%A4ufigsten_W%C3%B6rter_der_deutschen_Sprache\n",
    "stop_words = \"die, der, und, in, zu, den, das, nicht, von, sie, ist, des, sich, mit, dem, dass, er, es, ein, ich, auf, so, eine, auch, als, an, nach, wie, im, für\"\n",
    "stop_words += \"man, aber, aus, durch, wenn, nur, war, noch, werden, bei, hat, wir, was, wird, sein, einen, welche, sind, oder, zur, um, haben, einer, mir, über, ihm, diese, einem, ihr, uns\"\n",
    "stop_words += \"da, zum, kann, doch, vor, dieser, mich, ihn, du, hatte, seine, mehr, am, denn, nun, unter, sehr, selbst, schon, hier\"\n",
    "stop_words += \"bis, habe, ihre, dann, ihnen, seiner, alle, wieder, meine, Zeit, gegen, vom, ganz, einzelnen, wo, muss, ohne, eines, können, sei\"\n",
    "stop_words = stop_words.lower()\n",
    "stop_words = stop_words.split(', ')\n",
    "\n",
    "X = df.drop(['score', 'contr', 'vup', 'vdo'], axis=1)\n",
    "y = df['score']\n",
    "\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print(\"total data: \", len(X))\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, ylim=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title('Learning curve')\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel('Training examples'); plt.ylabel('Score')\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x156e932f0>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-c69d45f49e94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-224-7dc5fb9af5c1>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(estimator, X, y, ylim, n_jobs, train_sizes)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training examples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     train_sizes, train_scores, test_scores = learning_curve(\n\u001b[0;32m---> 23\u001b[0;31m         estimator, X, y, n_jobs=n_jobs, train_sizes=train_sizes)\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtrain_scores_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrain_scores_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[0;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             verbose, parameters=None, fit_params=None, return_train_score=True)\n\u001b[0;32m-> 1065\u001b[0;31m             for train, test in train_test_proportions)\n\u001b[0m\u001b[1;32m   1066\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0mn_cv_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mn_unique_ticks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    383\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                         \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 \u001b[0mCustomizablePickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reducers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x156e932f0>: attribute lookup <lambda> on __main__ failed"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEaCAYAAADtxAsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng4iIhDE0iIhIGEODiIiEMTSIiEjY/wEIHVUQb1mJzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12871b7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, explained_variance_score, mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Select a subset of data at a provided key.\n",
    "    key: hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    '''\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.key]\n",
    "    \n",
    "ngram_range = (1, 1)\n",
    "user_cols = [col for col in list(df.columns) if col.startswith('user_')]\n",
    "no_numbers = lambda x: re.sub(r'(\\d[\\d\\.])+', '', x.lower())\n",
    "model = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        [\n",
    "            # comment + user information\n",
    "            ('statistics', Pipeline([\n",
    "                ('selector', ItemSelector(\n",
    "                    key=['weekday', 'hour', 'con_len_trans', 'num_comments', \n",
    "                         'time_since_first_trans',# 'con_num_words',\n",
    "                          #'user_n_comments_trans', #'con_n_periods_trans',\n",
    "                        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "                         'cat_panorama', 'cat_community', 'cat_people', 'cat_digital'\n",
    "                        ])), #+ user_cols)),\n",
    "                # Polynomialfeatures can help a little bit...\n",
    "#                ('polynomialfeatures', PolynomialFeatures(degree=2)),\n",
    "            ])),\n",
    "\n",
    "            ('words_content', Pipeline([\n",
    "                ('selector', ItemSelector(key='con')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            # article information\n",
    "            ('words_title', Pipeline([\n",
    "                ('selector', ItemSelector(key='tit')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            ('words_subtitle', Pipeline([\n",
    "                ('selector', ItemSelector(key='sub')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=2, max_df=0.7, preprocessor=no_numbers, ngram_range=ngram_range)),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "            \n",
    "            ('words_text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=0.2, max_df=0.7, preprocessor=no_numbers, ngram_range=(1, 2))),\n",
    "                ('best', TruncatedSVD(n_components=30)),\n",
    "            ])),\n",
    "\n",
    "        ]\n",
    "    )),    \n",
    "    #('model', MLPRegressor(max_iter=50, hidden_layer_sizes=(100,)))\n",
    "    ('model', GradientBoostingRegressor(n_estimators=10, max_depth=2))\n",
    "])\n",
    "\n",
    "if True:\n",
    "    plot_learning_curve(model, X, y)\n",
    "    plt.show()\n",
    "else:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"R^2: %1.3f\" % r2_score(y_test, y_pred))\n",
    "    print(\"Explained var: {:3f}\".format(explained_variance_score(y_test, y_pred)))\n",
    "\n",
    "    # Residual plot\n",
    "    # time_since_first because it seems to have some influence (see feature importance below)\n",
    "    X_res = X_test['time_since_first_trans']\n",
    "    plt.scatter(X_res, y_test, color='black', label='test data')\n",
    "    plt.scatter(X_res, y_pred, color='red', label='predicted')\n",
    "    plt.xlabel('time_since_first_trans')\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "    if False:\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'model__hidden_layer_sizes':[(50,), (100,), (200,), (100, 50)]\n",
    "    #        'motdel__alpha': np.logspace(-1, 4, 6),\n",
    "                     }\n",
    "        grid = GridSearchCV(model, param_grid, cv=3)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary so far\n",
    "The decision tree regressor depends a lot on max_depth parameter: Depending on it, the model over- or underfits. I reached the best R^2 score with max_depth=3, but model underfits.\n",
    "\n",
    "Neural networks work a bit better: R^2 = 0.39 (old try)  \n",
    "GBRF same: R^2 = 0.45  \n",
    "ngram range (1, 2): 0.37  ngram range (1, 1): 0.366   \n",
    "adding more stop words: gives a very small increase\n",
    "\n",
    "The models improve slightly with more data, will try bigger runs.\n",
    "\n",
    "How to extract more information? How to use user behaviour?\n",
    "\n",
    "TODO: better text feature extration. Save trained model, make a prediction function where it's easy to input data to try around with score prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325\n",
      "325\n",
      "content\n",
      "[  3.36235444e-02   0.00000000e+00   0.00000000e+00   1.46943118e-02\n",
      "   5.39431393e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.52756075e-02   1.00141627e-02   0.00000000e+00\n",
      "   0.00000000e+00   6.19984651e-05   0.00000000e+00   2.21207406e-03\n",
      "   9.16771433e-03   0.00000000e+00   1.50467028e-03   0.00000000e+00\n",
      "   6.84321173e-03   3.16848264e-04   4.26150697e-03   0.00000000e+00\n",
      "   0.00000000e+00   1.18311252e-02   4.28463174e-03   2.89814845e-04\n",
      "   0.00000000e+00   4.17726172e-03   5.19424009e-03   2.56189331e-02\n",
      "   1.22325594e-02   7.79614393e-02   5.11375025e-03   3.02468885e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "tit\n",
      "[ 0.02023909  0.          0.00290409  0.          0.          0.03166067\n",
      "  0.          0.0021474   0.          0.0025421   0.          0.\n",
      "  0.02493418  0.          0.          0.00115153  0.00811151  0.\n",
      "  0.00852716  0.          0.00181089  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.00065992  0.        ]\n",
      "sub\n",
      "[  6.39214490e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   3.27425977e-05   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.62419906e-02\n",
      "   1.01450439e-02   0.00000000e+00   2.03574351e-02   1.11811036e-02\n",
      "   0.00000000e+00   2.33272433e-03   0.00000000e+00   0.00000000e+00\n",
      "   4.56829520e-02   0.00000000e+00   0.00000000e+00   1.43687054e-04\n",
      "   3.04759428e-02   1.12956338e-03   0.00000000e+00   0.00000000e+00\n",
      "   5.47635402e-03   0.00000000e+00]\n",
      "text\n",
      "[  8.63963663e-02   6.61135191e-03   0.00000000e+00   0.00000000e+00\n",
      "   1.45940878e-02   0.00000000e+00   0.00000000e+00   1.27871110e-02\n",
      "   9.08193150e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   3.53329722e-04   1.31764587e-02   2.33036494e-03   8.14229337e-03\n",
      "   3.52630577e-04   7.81327280e-04   2.54025924e-03   0.00000000e+00\n",
      "   0.00000000e+00   1.47239961e-02   3.09381416e-02   3.25247153e-04\n",
      "   1.71447237e-03   7.66113892e-03   1.60053122e-03   3.63389590e-05\n",
      "   6.52552915e-03   1.05601351e-02]\n"
     ]
    }
   ],
   "source": [
    "# Try to find out which features are not important\n",
    "# It seems that the text features are rather important\n",
    "\n",
    "fi = model.named_steps['model'].feature_importances_\n",
    "print(len(['weekday', 'hour', 'con_len_trans', 'num_comments', \n",
    "                         'time_since_first_trans',# 'con_num_words',\n",
    "                          'user_n_comments_trans', #'con_n_periods_trans',\n",
    "                        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "                         'cat_panorama', 'cat_community', 'cat_people', 'cat_digital'\n",
    "                       ] + user_cols))\n",
    "print(len(fi[:-130]))\n",
    "print('content')\n",
    "print(fi[-130:-90])\n",
    "print('tit')\n",
    "print(fi[-90:-60])\n",
    "print('sub')\n",
    "print(fi[-60:-30])\n",
    "print('text')\n",
    "print(fi[-30:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
