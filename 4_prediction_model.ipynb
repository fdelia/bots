{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction of User Reactions\n",
    "We want to predict:\n",
    "* How to get high score/consensus (upvotes - downvotes)\n",
    "* How to be controversial (upvotes + downvotes)  \n",
    "in the comments.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Not necessary, but I like the ggplot style better\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df_art = pd.read_csv('articles_2017_08.csv')\n",
    "df_com = pd.read_csv('comments_2017_08.csv').head(20000) # crop because battery life, skews data\n",
    "# Make float better readable\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        tId  article_id        updated  num_comments  \\\n",
      "0  18602624    18602624 1502366469.553             0   \n",
      "1  23276166    23276166 1502445817.487             0   \n",
      "2  16318631    16318631 1502461203.665            40   \n",
      "3  22118017    22118017 1502738791.517            22   \n",
      "4  21253641    21253641 1502287927.628           262   \n",
      "\n",
      "                                                link  \\\n",
      "0  /schweiz/ostschweiz/story/-Er-rannte-ihm-mit-d...   \n",
      "1                      /schweiz/basel/story/23276166   \n",
      "2     /community/dossier/geldratgeber/story/16318631   \n",
      "3  /schweiz/news/story/Zivis-sollen-bei-Fluechtli...   \n",
      "4                       /digital/news/story/21253641   \n",
      "\n",
      "                                             header  \\\n",
      "0       «Er rannte ihm mit der Mistgabel hinterher»   \n",
      "1  Unter Drogen und Alkohol Zugbegleiter verprügelt   \n",
      "2                Muss ich bei Vertragsbruch zahlen?   \n",
      "3        Zivis sollen bei Flüchtlingsansturm helfen   \n",
      "4   «In zehn Jahren gibt es keine Smartphones mehr»   \n",
      "\n",
      "                                                 sub  \\\n",
      "0  Ein ehemaliger Mitarbeiter erinnert sich an se...   \n",
      "1  von Adrian Jäggi - Unter dem Einfluss von Koka...   \n",
      "2  Marianna (27) schuldet ihrem Arbeitgeber eine ...   \n",
      "3  Zivildienstleistende sollen den Grenzwächtern ...   \n",
      "4  Wird das iPhone auch das 20-Jahr-Jubiläum feie...   \n",
      "\n",
      "                                                text   cat_copy  cat_ausland  \\\n",
      "0  Der heute 22-jährige Pferdepfleger Jonas M. (N...    schweiz            0   \n",
      "1  Der Angeklagte erinnerte sich am Donnerstag vo...    schweiz            0   \n",
      "2  Lieber Phil Geld Ich habe eine neue Arbeitsste...  community            0   \n",
      "3  Zivildienstleistende sollen in einem Pilotproj...    schweiz            0   \n",
      "4  Vor zehn Jahren wurde das erste iPhone verkauf...    digital            0   \n",
      "\n",
      "        ...        cat_native  cat_panorama  cat_people  cat_playview  \\\n",
      "0       ...                 0             0           0             0   \n",
      "1       ...                 0             0           0             0   \n",
      "2       ...                 0             0           0             0   \n",
      "3       ...                 0             0           0             0   \n",
      "4       ...                 0             0           0             0   \n",
      "\n",
      "   cat_schweiz  cat_sport  cat_wissen  header_len  text_len  text_num_words  \n",
      "0            1          0           0          43      4462             645  \n",
      "1            1          0           0          48      2496             377  \n",
      "2            0          0           0          34       206              29  \n",
      "3            1          0           0          42      1666             228  \n",
      "4            0          0           0          47      1870             268  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_dt_obj(time):\n",
    "    time = time.replace('am ', '')\n",
    "    # Make datetime object from string\n",
    "    return datetime.strptime(time, '%d.%m.%Y %H:%M')\n",
    "\n",
    "def time_since_epoch(time):\n",
    "    return (get_dt_obj(time)-datetime(1970,1,1)).total_seconds()\n",
    "\n",
    "def get_hour_of_day(time):\n",
    "    return get_dt_obj(time).hour\n",
    "\n",
    "def get_weekday(time):\n",
    "    return get_dt_obj(time).weekday()\n",
    "\n",
    "# Basically same as \"the hour of week\" or \"weekday_hourOfDay\"\n",
    "def get_weekday_float(time):\n",
    "    hour = float(get_hour_of_day(time))\n",
    "    weekday = get_weekday(time)\n",
    "    return float(weekday) + hour / 24\n",
    "\n",
    "def get_weekday_hour(time):\n",
    "    return '{}_{}'.format(get_weekday(time), get_hour_of_day(time))\n",
    "\n",
    "df_com['time_since_epoch'] = df_com['time'].apply(time_since_epoch)\n",
    "df_com['hour'] = df_com['time'].apply(get_hour_of_day)\n",
    "df_com['weekday'] = df_com['time'].apply(get_weekday) # 0 = Monday\n",
    "df_com['weekday_fl'] = df_com['time'].apply(get_weekday_float)\n",
    "df_com['weekday_hour'] = df_com['time'].apply(get_weekday_hour)\n",
    "df_com['is_answer'] = df_com['tit'].apply(lambda x: str(x).startswith('@'))\n",
    "df_com['con_len'] = df_com['con'].apply(lambda x: len(x))\n",
    "df_com['con_num_words'] = df_com['con'].apply(lambda x: len(x.split()))\n",
    "df_com['score'] = df_com['vup'] - df_com['vdo']\n",
    "df_com['contr'] = df_com['vup'] + df_com['vdo']\n",
    "\n",
    "df_com['tit'] = df_com['tit'].str.lower()\n",
    "df_com['con'] = df_com['con'].str.lower()\n",
    "\n",
    "def get_category(link):\n",
    "    t = link.split('/')\n",
    "    if len(t) <= 1:\n",
    "        return ''\n",
    "    else:\n",
    "        return t[1]\n",
    "\n",
    "df_art['cat'] = df_art['link'].apply(get_category)\n",
    "df_art['cat_copy'] = df_art['cat']\n",
    "df_art = pd.get_dummies(df_art, columns=['cat'])\n",
    "\n",
    "df_art['header_len'] = df_art['header'].apply(lambda x: len(x))\n",
    "df_art['text_len'] = df_art['text'].apply(lambda x: len(str(x)))\n",
    "df_art['text_num_words'] = df_art['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Left inner join\n",
    "df_merge = pd.merge(left=df_com, right=df_art, left_on='tId', right_on='tId')\n",
    "\n",
    "# is empty: (= all comments correspond to an article)\n",
    "#print(df_merge[pd.isnull(df_merge['link'])])\n",
    "#df_merge[df_merge['vup']>1000]\n",
    "\n",
    "# Get order of comments per article\n",
    "df_merge_art = df_merge.sort_values(['tId', 'time_since_epoch']).groupby('tId')\n",
    "# Get time since the first comment\n",
    "def get_time_since_first(group):\n",
    "    first = group.iloc[:1]['time_since_epoch']\n",
    "    group['time_since_first'] = group['time_since_epoch'].apply(lambda x: (x - first) / 3600)\n",
    "    # Remove those very late comments, after x hours\n",
    "    #group = group[group['time_since_first'] < 36]\n",
    "    return group\n",
    "\n",
    "df_merge_art = df_merge_art.apply(get_time_since_first)\n",
    "print(df_art.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to remove skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_cat\n",
      "negative    6663\n",
      "small       7432\n",
      "big         6139\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score_trans</th>\n",
       "      <th>vup</th>\n",
       "      <th>vdo</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday_fl</th>\n",
       "      <th>con_len_trans</th>\n",
       "      <th>text_len_trans</th>\n",
       "      <th>con_num_words_trans</th>\n",
       "      <th>text_num_words_trans</th>\n",
       "      <th>time_since_first_trans</th>\n",
       "      <th>header_len_trans</th>\n",
       "      <th>cat_schweiz</th>\n",
       "      <th>cat_finance</th>\n",
       "      <th>cat_sport</th>\n",
       "      <th>cat_wissen</th>\n",
       "      <th>cat_ausland</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score_trans</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vup</th>\n",
       "      <td>0.355</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.408</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vdo</th>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.408</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_fl</th>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.151</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_len_trans</th>\n",
       "      <td>0.044</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len_trans</th>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.994</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_num_words_trans</th>\n",
       "      <td>0.041</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.059</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_num_words_trans</th>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.058</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_since_first_trans</th>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>header_len_trans</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_schweiz</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.103</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_finance</th>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_sport</th>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_wissen</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_ausland</th>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        score_trans    vup    vdo  weekday   hour  weekday_fl  \\\n",
       "score_trans                   1.000  0.355 -0.311   -0.018 -0.015      -0.019   \n",
       "vup                           0.355  1.000  0.408   -0.008 -0.012      -0.009   \n",
       "vdo                          -0.311  0.408  1.000    0.012  0.000       0.012   \n",
       "weekday                      -0.018 -0.008  0.012    1.000  0.036       0.993   \n",
       "hour                         -0.015 -0.012  0.000    0.036  1.000       0.151   \n",
       "weekday_fl                   -0.019 -0.009  0.012    0.993  0.151       1.000   \n",
       "con_len_trans                 0.044  0.010  0.025   -0.007 -0.065      -0.014   \n",
       "text_len_trans               -0.017 -0.010 -0.007   -0.003 -0.051      -0.009   \n",
       "con_num_words_trans           0.041  0.008  0.023   -0.009 -0.064      -0.016   \n",
       "text_num_words_trans         -0.017 -0.010 -0.006   -0.009 -0.055      -0.016   \n",
       "time_since_first_trans       -0.183 -0.223 -0.177    0.063 -0.061       0.056   \n",
       "header_len_trans             -0.005  0.005  0.016   -0.038  0.026      -0.034   \n",
       "cat_schweiz                   0.025  0.055  0.045    0.010 -0.072       0.002   \n",
       "cat_finance                  -0.032 -0.051 -0.044    0.004 -0.015       0.003   \n",
       "cat_sport                    -0.026 -0.010  0.018    0.009  0.009       0.010   \n",
       "cat_wissen                   -0.002 -0.022 -0.038    0.001  0.056       0.007   \n",
       "cat_ausland                  -0.008  0.008  0.029   -0.037  0.001      -0.037   \n",
       "\n",
       "                        con_len_trans  text_len_trans  con_num_words_trans  \\\n",
       "score_trans                     0.044          -0.017                0.041   \n",
       "vup                             0.010          -0.010                0.008   \n",
       "vdo                             0.025          -0.007                0.023   \n",
       "weekday                        -0.007          -0.003               -0.009   \n",
       "hour                           -0.065          -0.051               -0.064   \n",
       "weekday_fl                     -0.014          -0.009               -0.016   \n",
       "con_len_trans                   1.000           0.062                0.989   \n",
       "text_len_trans                  0.062           1.000                0.059   \n",
       "con_num_words_trans             0.989           0.059                1.000   \n",
       "text_num_words_trans            0.061           0.994                0.058   \n",
       "time_since_first_trans          0.043          -0.062                0.045   \n",
       "header_len_trans                0.005           0.063                0.003   \n",
       "cat_schweiz                     0.062           0.165                0.060   \n",
       "cat_finance                     0.032           0.050                0.029   \n",
       "cat_sport                      -0.056          -0.004               -0.052   \n",
       "cat_wissen                     -0.022           0.028               -0.023   \n",
       "cat_ausland                     0.022           0.076                0.017   \n",
       "\n",
       "                        text_num_words_trans  time_since_first_trans  \\\n",
       "score_trans                           -0.017                  -0.183   \n",
       "vup                                   -0.010                  -0.223   \n",
       "vdo                                   -0.006                  -0.177   \n",
       "weekday                               -0.009                   0.063   \n",
       "hour                                  -0.055                  -0.061   \n",
       "weekday_fl                            -0.016                   0.056   \n",
       "con_len_trans                          0.061                   0.043   \n",
       "text_len_trans                         0.994                  -0.062   \n",
       "con_num_words_trans                    0.058                   0.045   \n",
       "text_num_words_trans                   1.000                  -0.066   \n",
       "time_since_first_trans                -0.066                   1.000   \n",
       "header_len_trans                       0.065                  -0.022   \n",
       "cat_schweiz                            0.171                  -0.129   \n",
       "cat_finance                            0.036                   0.087   \n",
       "cat_sport                             -0.001                  -0.006   \n",
       "cat_wissen                             0.027                   0.012   \n",
       "cat_ausland                            0.072                   0.028   \n",
       "\n",
       "                        header_len_trans  cat_schweiz  cat_finance  cat_sport  \\\n",
       "score_trans                       -0.005        0.025       -0.032     -0.026   \n",
       "vup                                0.005        0.055       -0.051     -0.010   \n",
       "vdo                                0.016        0.045       -0.044      0.018   \n",
       "weekday                           -0.038        0.010        0.004      0.009   \n",
       "hour                               0.026       -0.072       -0.015      0.009   \n",
       "weekday_fl                        -0.034        0.002        0.003      0.010   \n",
       "con_len_trans                      0.005        0.062        0.032     -0.056   \n",
       "text_len_trans                     0.063        0.165        0.050     -0.004   \n",
       "con_num_words_trans                0.003        0.060        0.029     -0.052   \n",
       "text_num_words_trans               0.065        0.171        0.036     -0.001   \n",
       "time_since_first_trans            -0.022       -0.129        0.087     -0.006   \n",
       "header_len_trans                   1.000        0.103       -0.012     -0.139   \n",
       "cat_schweiz                        0.103        1.000       -0.357     -0.246   \n",
       "cat_finance                       -0.012       -0.357        1.000     -0.122   \n",
       "cat_sport                         -0.139       -0.246       -0.122      1.000   \n",
       "cat_wissen                         0.014       -0.270       -0.134     -0.092   \n",
       "cat_ausland                       -0.002       -0.203       -0.101     -0.069   \n",
       "\n",
       "                        cat_wissen  cat_ausland  \n",
       "score_trans                 -0.002       -0.008  \n",
       "vup                         -0.022        0.008  \n",
       "vdo                         -0.038        0.029  \n",
       "weekday                      0.001       -0.037  \n",
       "hour                         0.056        0.001  \n",
       "weekday_fl                   0.007       -0.037  \n",
       "con_len_trans               -0.022        0.022  \n",
       "text_len_trans               0.028        0.076  \n",
       "con_num_words_trans         -0.023        0.017  \n",
       "text_num_words_trans         0.027        0.072  \n",
       "time_since_first_trans       0.012        0.028  \n",
       "header_len_trans             0.014       -0.002  \n",
       "cat_schweiz                 -0.270       -0.203  \n",
       "cat_finance                 -0.134       -0.101  \n",
       "cat_sport                   -0.092       -0.069  \n",
       "cat_wissen                   1.000       -0.076  \n",
       "cat_ausland                 -0.076        1.000  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def show_hist(col):\n",
    "    df_merge_art['bins'] = pd.cut(df_merge_art[col], 100)\n",
    "    h = df_merge_art.groupby(['bins']).size().plot.bar(title=col)\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    \n",
    "# This categories are random\n",
    "bins = [-np.inf, 2, 25, np.inf]\n",
    "labels = ['negative', 'small', 'big']\n",
    "df_merge_art['score_cat'] = pd.cut(df_merge_art['score'], bins, labels=labels)\n",
    "print(df_merge_art.groupby('score_cat').size())\n",
    "\n",
    "#df = df[(df['score']<2) | (df['score']>=25)]\n",
    "    \n",
    "df = df_merge_art.copy(deep=True)\n",
    "def sgn(x):\n",
    "    if x == 0: return 0\n",
    "    else: return x/abs(x)\n",
    "# Removes left/right skew \n",
    "transformation = lambda x: sgn(x)*math.log(abs(x) + 1)\n",
    "for col in ['weekday_fl', 'con_len', 'text_len', 'time_since_first', \n",
    "            'score', 'contr', 'header_len', 'con_num_words', 'text_num_words']:\n",
    "    #min_val = df_merge_art[col].min()\n",
    "    df[col + '_trans'] = df_merge_art[col].apply(transformation)\n",
    "    #show_hist(col + '_trans')\n",
    "\n",
    "# I removed very weak correlations to \"score\" and \"contr\"\n",
    "cols = ['score_trans', 'vup', 'vdo', 'weekday', 'hour', 'weekday_fl',\n",
    "       'con_len_trans', 'text_len_trans', 'con_num_words_trans', \n",
    "        'text_num_words_trans',\n",
    "       'time_since_first_trans', 'header_len_trans', \n",
    "        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland']\n",
    "# Get pearson co-efficients\n",
    "df[cols].corr()\n",
    "\n",
    "#for c in cols:\n",
    "#    print(c)\n",
    "#    print(df[np.isnan(df[col])].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and split for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://de.wikipedia.org/wiki/Liste_der_h%C3%A4ufigsten_W%C3%B6rter_der_deutschen_Sprache\n",
    "stop_words = \"die, der, und, in, zu, den, das, nicht, von, sie, ist, des, sich, mit, dem, dass, er, es, ein, ich, auf, so, eine, auch, als, an, nach, wie, im, für\"\n",
    "#stop_words += \"man, aber, aus, durch, wenn, nur, war, noch, werden, bei, hat, wir, was, wird, sein, einen, welche, sind, oder, zur, um, haben, einer, mir, über, ihm, diese, einem, ihr, uns\"\n",
    "#stop_words += \"da, zum, kann, doch, vor, dieser, mich, ihn, du, hatte, seine, mehr, am, denn, nun, unter, sehr, selbst, schon, hier\"\n",
    "#stop_words += \"bis, habe, ihre, dann, ihnen, seiner, alle, wieder, meine, Zeit, gegen, vom, ganz, einzelnen, wo, muss, ohne, eines, können, sei\"\n",
    "stop_words = stop_words.lower()\n",
    "stop_words = stop_words.split(', ')\n",
    "\n",
    "def classify(val):\n",
    "    if val > 10: return 1\n",
    "    return 0\n",
    "\n",
    "X = df.drop(['score', 'contr', 'vup', 'vdo'], axis=1)\n",
    "\n",
    "#y = df_merge_art['score'].apply(classify)\n",
    "y = df['score']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.044\n",
      "\n",
      "[ 15.31593537  60.90306307   3.52024258 ...,  18.99753345   2.20516528\n",
      "  52.59222045]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "#from sklearn.feature_selection import SelectKBest, SelectPercentile, f_regression\n",
    "#from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Select a subset of data at a provided key.\n",
    "    key: hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    '''\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.key]\n",
    "    \n",
    "# TODO onehotencoder for \"cat\"\n",
    "no_numbers = lambda x: re.sub(r'(\\d[\\d\\.])+', '', x.lower())\n",
    "model = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=\n",
    "        [\n",
    "            ('statistics', Pipeline([\n",
    "                ('selector', ItemSelector(\n",
    "                    key=['weekday', 'hour', 'con_len_trans', \n",
    "                         'time_since_first_trans',\n",
    "                        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "                         'cat_panorama', 'cat_community', 'cat_people', 'cat_digital'\n",
    "                        ])),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('polynomialfeatures', PolynomialFeatures(degree=3))\n",
    "            ]))#,\n",
    "            \n",
    "            #('categories', Pipeline([\n",
    "            #    ('selector', ItemSelector(key=['cat'])),\n",
    "            #    ('encoder', OneHotEncoder())\n",
    "            #]))\n",
    "            \n",
    "            # Course of dimensionality!\n",
    "         #   ('words', Pipeline([\n",
    "          #      ('selector', ItemSelector(key='con')),\n",
    "            #    ('vect', CountVectorizer(preprocessor=no_numbers, ngram_range=(1, 1), stop_words=stop_words))\n",
    "           #     ('sent_model', sent_model)\n",
    "           # ]))\n",
    "        ],\n",
    "        \n",
    "        transformer_weights={\n",
    "            'statistics': .8,\n",
    "            'words': .2\n",
    "        }\n",
    "    )),\n",
    "    \n",
    "    # Params not optimized yet\n",
    "    #('model', ElasticNet(alpha=1e-1, l1_ratio=0.3, max_iter=50))\n",
    "    ('model', Ridge())\n",
    "    #('model', SVC(kernel='linear', n_jobs=-1))\n",
    "])\n",
    "\n",
    "#model = make_pipeline(ItemSelector(\n",
    "#        key=['weekday', 'hour', 'time_since_first', 'con_len']),\n",
    "#        StandardScaler(), PolynomialFeatures(degree=5), Ridge(alpha=100, max_iter=10, solver='cholesky'))\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R^2: %1.3f\\n\" % r2_score(y_test, y_pred))\n",
    "print(y_pred)\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "if True:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_grid = {#'polynomialfeatures__degree': [2, 3, 4, 5],\n",
    "        'model__alpha': np.logspace(-1, 3, 5),\n",
    "#        'model__l1_ratio': [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "        'model__max_iter': [10, 50, 100, 500],\n",
    "        'model__solver': ['auto', 'svd', 'cholesky', 'lsqr']\n",
    "                 }\n",
    "    grid = GridSearchCV(model, param_grid, cv=3)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best estimator:\\n{}\".format(grid.best_estimator_.named_steps['model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
