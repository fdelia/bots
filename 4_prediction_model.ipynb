{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction of User Reactions\n",
    "We want to predict:\n",
    "* How to get high score/consensus (upvotes - downvotes)\n",
    "* How to be controversial (upvotes + downvotes)  \n",
    "in the comments.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Not necessary, but I like the ggplot style better\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df_art = pd.read_csv('articles_2017_08.csv')\n",
    "df_com = pd.read_csv('comments_2017_08.csv').sample(20000) # crop because battery life, skews data\n",
    "# Make float better readable\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        tId  article_id        updated  num_comments  \\\n",
      "0  18602624    18602624 1502366469.553             0   \n",
      "1  23276166    23276166 1502975307.473             0   \n",
      "2  17735228    17735228 1502796186.031            29   \n",
      "3  16318631    16318631 1502461203.665            40   \n",
      "4  22118017    22118017 1502796053.100            20   \n",
      "\n",
      "                                                link  \\\n",
      "0  /schweiz/ostschweiz/story/-Er-rannte-ihm-mit-d...   \n",
      "1                      /schweiz/basel/story/23276166   \n",
      "2  /schweiz/zuerich/story/Limmatschwimmen-findet-...   \n",
      "3     /community/dossier/geldratgeber/story/16318631   \n",
      "4  /schweiz/news/story/Zivis-sollen-bei-Fluechtli...   \n",
      "\n",
      "                                             header  \\\n",
      "0       «Er rannte ihm mit der Mistgabel hinterher»   \n",
      "1  Unter Drogen und Alkohol Zugbegleiter verprügelt   \n",
      "2     Limmatschwimmen findet am Samstag nicht statt   \n",
      "3                Muss ich bei Vertragsbruch zahlen?   \n",
      "4        Zivis sollen bei Flüchtlingsansturm helfen   \n",
      "\n",
      "                                                 sub  \\\n",
      "0  Ein ehemaliger Mitarbeiter erinnert sich an se...   \n",
      "1  von Adrian Jäggi - Unter dem Einfluss von Koka...   \n",
      "2  Der Zürcher Traditionsanlass musste verschoben...   \n",
      "3  Marianna (27) schuldet ihrem Arbeitgeber eine ...   \n",
      "4  Zivildienstleistende sollen den Grenzwächtern ...   \n",
      "\n",
      "                                                text   cat_copy  cat_ausland  \\\n",
      "0  Der heute 22-jährige Pferdepfleger Jonas M. (N...    schweiz            0   \n",
      "1  Der Angeklagte erinnerte sich am Donnerstag vo...    schweiz            0   \n",
      "2  Das Zürcher Limmatschwimmen findet am kommende...    schweiz            0   \n",
      "3  Lieber Phil Geld Ich habe eine neue Arbeitsste...  community            0   \n",
      "4  Zivildienstleistende sollen in einem Pilotproj...    schweiz            0   \n",
      "\n",
      "        ...        cat_native  cat_panorama  cat_people  cat_playview  \\\n",
      "0       ...                 0             0           0             0   \n",
      "1       ...                 0             0           0             0   \n",
      "2       ...                 0             0           0             0   \n",
      "3       ...                 0             0           0             0   \n",
      "4       ...                 0             0           0             0   \n",
      "\n",
      "   cat_schweiz  cat_sport  cat_wissen  header_len  text_len  text_num_words  \n",
      "0            1          0           0          43      4462             645  \n",
      "1            1          0           0          48      2496             377  \n",
      "2            1          0           0          45      1385             191  \n",
      "3            0          0           0          34       206              29  \n",
      "4            1          0           0          42      1666             228  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_dt_obj(time):\n",
    "    time = time.replace('am ', '')\n",
    "    # Make datetime object from string\n",
    "    return datetime.strptime(time, '%d.%m.%Y %H:%M')\n",
    "\n",
    "def time_since_epoch(time):\n",
    "    return (get_dt_obj(time)-datetime(1970,1,1)).total_seconds()\n",
    "\n",
    "def get_hour_of_day(time):\n",
    "    return get_dt_obj(time).hour\n",
    "\n",
    "def get_weekday(time):\n",
    "    return get_dt_obj(time).weekday()\n",
    "\n",
    "# Basically same as \"the hour of week\" or \"weekday_hourOfDay\"\n",
    "def get_weekday_float(time):\n",
    "    hour = float(get_hour_of_day(time))\n",
    "    weekday = get_weekday(time)\n",
    "    return float(weekday) + hour / 24\n",
    "\n",
    "def get_weekday_hour(time):\n",
    "    return '{}_{}'.format(get_weekday(time), get_hour_of_day(time))\n",
    "\n",
    "df_com['time_since_epoch'] = df_com['time'].apply(time_since_epoch)\n",
    "df_com['hour'] = df_com['time'].apply(get_hour_of_day)\n",
    "df_com['weekday'] = df_com['time'].apply(get_weekday) # 0 = Monday\n",
    "df_com['weekday_fl'] = df_com['time'].apply(get_weekday_float)\n",
    "df_com['weekday_hour'] = df_com['time'].apply(get_weekday_hour)\n",
    "df_com['is_answer'] = df_com['tit'].apply(lambda x: str(x).startswith('@'))\n",
    "df_com['con_len'] = df_com['con'].apply(lambda x: len(x))\n",
    "df_com['con_num_words'] = df_com['con'].apply(lambda x: len(x.split()))\n",
    "df_com['score'] = df_com['vup'] - df_com['vdo']\n",
    "df_com['contr'] = df_com['vup'] + df_com['vdo']\n",
    "\n",
    "df_com['tit'] = df_com['tit'].str.lower()\n",
    "df_com['con'] = df_com['con'].str.lower()\n",
    "\n",
    "def get_category(link):\n",
    "    t = link.split('/')\n",
    "    if len(t) <= 1:\n",
    "        return ''\n",
    "    else:\n",
    "        return t[1]\n",
    "\n",
    "df_art['cat'] = df_art['link'].apply(get_category)\n",
    "df_art['cat_copy'] = df_art['cat']\n",
    "df_art = pd.get_dummies(df_art, columns=['cat'])\n",
    "\n",
    "df_art['header_len'] = df_art['header'].apply(lambda x: len(x))\n",
    "df_art['text_len'] = df_art['text'].apply(lambda x: len(str(x)))\n",
    "df_art['text_num_words'] = df_art['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Left inner join\n",
    "df_merge = pd.merge(left=df_com, right=df_art, left_on='tId', right_on='tId')\n",
    "\n",
    "# is empty: (= all comments correspond to an article)\n",
    "#print(df_merge[pd.isnull(df_merge['link'])])\n",
    "#df_merge[df_merge['vup']>1000]\n",
    "\n",
    "# Get order of comments per article\n",
    "df_merge_art = df_merge.sort_values(['tId', 'time_since_epoch']).groupby('tId')\n",
    "# Get time since the first comment\n",
    "def get_time_since_first(group):\n",
    "    first = group.iloc[:1]['time_since_epoch']\n",
    "    group['time_since_first'] = group['time_since_epoch'].apply(lambda x: (x - first) / 3600)\n",
    "    # Remove those very late comments, after x hours\n",
    "    #group = group[group['time_since_first'] < 36]\n",
    "    return group\n",
    "\n",
    "df_merge_art = df_merge_art.apply(get_time_since_first)\n",
    "print(df_art.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to remove skew\n",
    "__Rescaling__: Add or subtract a constant and then multiply or divide by a constant.  \n",
    "__Normalizing__: Dividing by a norm of the vector, e.g. make Euclidean length equal to one. Sometimes make all elements lie in [0, 1].  \n",
    "__Standardizing__: Subtracting a measure of location and dividing by a measure of scale. Eg. subtract the mean and divide by the std, thereby obtaining a standard normal distribution.\n",
    "\n",
    "These terms are sometimes used interchangeably.\n",
    "\n",
    "It's usually better to have the input values centered around zero, unless the output activation function has a range of [0, 1] (neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_cat\n",
      "negative    6615\n",
      "small       7651\n",
      "big         5983\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>score_trans</th>\n",
       "      <th>vup</th>\n",
       "      <th>vdo</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday_fl</th>\n",
       "      <th>con_len_trans</th>\n",
       "      <th>text_len_trans</th>\n",
       "      <th>con_num_words_trans</th>\n",
       "      <th>text_num_words_trans</th>\n",
       "      <th>time_since_first_trans</th>\n",
       "      <th>header_len_trans</th>\n",
       "      <th>cat_schweiz</th>\n",
       "      <th>cat_finance</th>\n",
       "      <th>cat_sport</th>\n",
       "      <th>cat_wissen</th>\n",
       "      <th>cat_ausland</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_trans</th>\n",
       "      <td>0.496</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vup</th>\n",
       "      <td>0.955</td>\n",
       "      <td>0.368</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vdo</th>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.356</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_fl</th>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.126</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_len_trans</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len_trans</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.073</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.994</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_num_words_trans</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.068</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_num_words_trans</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.068</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_since_first_trans</th>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>header_len_trans</th>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_schweiz</th>\n",
       "      <td>0.044</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_finance</th>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_sport</th>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_wissen</th>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_ausland</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        score  score_trans    vup    vdo  weekday   hour  \\\n",
       "score                   1.000        0.496  0.955  0.061   -0.018 -0.020   \n",
       "score_trans             0.496        1.000  0.368 -0.325   -0.013 -0.017   \n",
       "vup                     0.955        0.368  1.000  0.356   -0.018 -0.018   \n",
       "vdo                     0.061       -0.325  0.356  1.000   -0.004  0.001   \n",
       "weekday                -0.018       -0.013 -0.018 -0.004    1.000  0.008   \n",
       "hour                   -0.020       -0.017 -0.018  0.001    0.008  1.000   \n",
       "weekday_fl             -0.020       -0.015 -0.020 -0.003    0.993  0.126   \n",
       "con_len_trans           0.006        0.036  0.013  0.025    0.002 -0.048   \n",
       "text_len_trans          0.003        0.000  0.001 -0.007   -0.015 -0.025   \n",
       "con_num_words_trans     0.003        0.032  0.010  0.025    0.003 -0.049   \n",
       "text_num_words_trans    0.004        0.001  0.001 -0.006   -0.023 -0.027   \n",
       "time_since_first_trans -0.184       -0.173 -0.223 -0.170    0.099 -0.068   \n",
       "header_len_trans        0.011       -0.000  0.010  0.001   -0.049  0.022   \n",
       "cat_schweiz             0.044        0.035  0.046  0.018    0.000 -0.055   \n",
       "cat_finance            -0.030       -0.020 -0.043 -0.048   -0.015 -0.007   \n",
       "cat_sport              -0.023       -0.025 -0.013  0.029    0.010  0.004   \n",
       "cat_wissen             -0.014       -0.010 -0.019 -0.020    0.012  0.032   \n",
       "cat_ausland            -0.001       -0.016  0.010  0.037   -0.024  0.006   \n",
       "\n",
       "                        weekday_fl  con_len_trans  text_len_trans  \\\n",
       "score                       -0.020          0.006           0.003   \n",
       "score_trans                 -0.015          0.036           0.000   \n",
       "vup                         -0.020          0.013           0.001   \n",
       "vdo                         -0.003          0.025          -0.007   \n",
       "weekday                      0.993          0.002          -0.015   \n",
       "hour                         0.126         -0.048          -0.025   \n",
       "weekday_fl                   1.000         -0.004          -0.018   \n",
       "con_len_trans               -0.004          1.000           0.073   \n",
       "text_len_trans              -0.018          0.073           1.000   \n",
       "con_num_words_trans         -0.003          0.989           0.068   \n",
       "text_num_words_trans        -0.026          0.072           0.994   \n",
       "time_since_first_trans       0.090          0.050          -0.053   \n",
       "header_len_trans            -0.046          0.020           0.066   \n",
       "cat_schweiz                 -0.006          0.048           0.165   \n",
       "cat_finance                 -0.016          0.046           0.074   \n",
       "cat_sport                    0.010         -0.054          -0.008   \n",
       "cat_wissen                   0.015         -0.013           0.015   \n",
       "cat_ausland                 -0.024          0.014           0.062   \n",
       "\n",
       "                        con_num_words_trans  text_num_words_trans  \\\n",
       "score                                 0.003                 0.004   \n",
       "score_trans                           0.032                 0.001   \n",
       "vup                                   0.010                 0.001   \n",
       "vdo                                   0.025                -0.006   \n",
       "weekday                               0.003                -0.023   \n",
       "hour                                 -0.049                -0.027   \n",
       "weekday_fl                           -0.003                -0.026   \n",
       "con_len_trans                         0.989                 0.072   \n",
       "text_len_trans                        0.068                 0.994   \n",
       "con_num_words_trans                   1.000                 0.068   \n",
       "text_num_words_trans                  0.068                 1.000   \n",
       "time_since_first_trans                0.051                -0.057   \n",
       "header_len_trans                      0.017                 0.074   \n",
       "cat_schweiz                           0.045                 0.170   \n",
       "cat_finance                           0.042                 0.061   \n",
       "cat_sport                            -0.050                -0.004   \n",
       "cat_wissen                           -0.012                 0.013   \n",
       "cat_ausland                           0.009                 0.055   \n",
       "\n",
       "                        time_since_first_trans  header_len_trans  cat_schweiz  \\\n",
       "score                                   -0.184             0.011        0.044   \n",
       "score_trans                             -0.173            -0.000        0.035   \n",
       "vup                                     -0.223             0.010        0.046   \n",
       "vdo                                     -0.170             0.001        0.018   \n",
       "weekday                                  0.099            -0.049        0.000   \n",
       "hour                                    -0.068             0.022       -0.055   \n",
       "weekday_fl                               0.090            -0.046       -0.006   \n",
       "con_len_trans                            0.050             0.020        0.048   \n",
       "text_len_trans                          -0.053             0.066        0.165   \n",
       "con_num_words_trans                      0.051             0.017        0.045   \n",
       "text_num_words_trans                    -0.057             0.074        0.170   \n",
       "time_since_first_trans                   1.000            -0.005       -0.086   \n",
       "header_len_trans                        -0.005             1.000        0.067   \n",
       "cat_schweiz                             -0.086             0.067        1.000   \n",
       "cat_finance                              0.048             0.031       -0.395   \n",
       "cat_sport                               -0.014            -0.138       -0.232   \n",
       "cat_wissen                               0.032             0.011       -0.268   \n",
       "cat_ausland                              0.010             0.009       -0.219   \n",
       "\n",
       "                        cat_finance  cat_sport  cat_wissen  cat_ausland  \n",
       "score                        -0.030     -0.023      -0.014       -0.001  \n",
       "score_trans                  -0.020     -0.025      -0.010       -0.016  \n",
       "vup                          -0.043     -0.013      -0.019        0.010  \n",
       "vdo                          -0.048      0.029      -0.020        0.037  \n",
       "weekday                      -0.015      0.010       0.012       -0.024  \n",
       "hour                         -0.007      0.004       0.032        0.006  \n",
       "weekday_fl                   -0.016      0.010       0.015       -0.024  \n",
       "con_len_trans                 0.046     -0.054      -0.013        0.014  \n",
       "text_len_trans                0.074     -0.008       0.015        0.062  \n",
       "con_num_words_trans           0.042     -0.050      -0.012        0.009  \n",
       "text_num_words_trans          0.061     -0.004       0.013        0.055  \n",
       "time_since_first_trans        0.048     -0.014       0.032        0.010  \n",
       "header_len_trans              0.031     -0.138       0.011        0.009  \n",
       "cat_schweiz                  -0.395     -0.232      -0.268       -0.219  \n",
       "cat_finance                   1.000     -0.118      -0.136       -0.111  \n",
       "cat_sport                    -0.118      1.000      -0.080       -0.065  \n",
       "cat_wissen                   -0.136     -0.080       1.000       -0.075  \n",
       "cat_ausland                  -0.111     -0.065      -0.075        1.000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def show_hist(col):\n",
    "    df_merge_art['bins'] = pd.cut(df_merge_art[col], 100)\n",
    "    h = df_merge_art.groupby(['bins']).size().plot.bar(title=col)\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    \n",
    "# This categories are random\n",
    "bins = [-np.inf, 2, 25, np.inf]\n",
    "labels = ['negative', 'small', 'big']\n",
    "df_merge_art['score_cat'] = pd.cut(df_merge_art['score'], bins, labels=labels)\n",
    "print(df_merge_art.groupby('score_cat').size())\n",
    "\n",
    "# This doesn't change anything, which is interesting\n",
    "#df = df[(df['score']<2) | (df['score']>=25)]\n",
    "    \n",
    "df = df_merge_art.copy(deep=True)\n",
    "def sgn(x):\n",
    "    if x == 0: return 0\n",
    "    else: return x/abs(x)\n",
    "# Removes left/right skew \n",
    "transformation = lambda x: sgn(x)*math.log(abs(x) + 1)\n",
    "for col in ['weekday_fl', 'con_len', 'text_len', 'time_since_first', \n",
    "            'score', 'contr', 'header_len', 'con_num_words', 'text_num_words']:\n",
    "    #min_val = df_merge_art[col].min()\n",
    "    df[col + '_trans'] = df_merge_art[col].apply(transformation)\n",
    "    #show_hist(col + '_trans')\n",
    "\n",
    "# I removed very weak correlations to \"score\" and \"contr\"\n",
    "cols = ['score', 'score_trans', 'vup', 'vdo', 'weekday', 'hour', 'weekday_fl',\n",
    "       'con_len_trans', 'text_len_trans', 'con_num_words_trans', \n",
    "        'text_num_words_trans',\n",
    "       'time_since_first_trans', 'header_len_trans', \n",
    "        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland']\n",
    "# Get pearson co-efficients\n",
    "df[cols].corr()\n",
    "\n",
    "#for c in cols:\n",
    "#    print(c)\n",
    "#    print(df[np.isnan(df[col])].head(3))\n",
    "\n",
    "# TODO transform data for learners (non-linear probably)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and split for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://de.wikipedia.org/wiki/Liste_der_h%C3%A4ufigsten_W%C3%B6rter_der_deutschen_Sprache\n",
    "stop_words = \"die, der, und, in, zu, den, das, nicht, von, sie, ist, des, sich, mit, dem, dass, er, es, ein, ich, auf, so, eine, auch, als, an, nach, wie, im, für\"\n",
    "#stop_words += \"man, aber, aus, durch, wenn, nur, war, noch, werden, bei, hat, wir, was, wird, sein, einen, welche, sind, oder, zur, um, haben, einer, mir, über, ihm, diese, einem, ihr, uns\"\n",
    "#stop_words += \"da, zum, kann, doch, vor, dieser, mich, ihn, du, hatte, seine, mehr, am, denn, nun, unter, sehr, selbst, schon, hier\"\n",
    "#stop_words += \"bis, habe, ihre, dann, ihnen, seiner, alle, wieder, meine, Zeit, gegen, vom, ganz, einzelnen, wo, muss, ohne, eines, können, sei\"\n",
    "stop_words = stop_words.lower()\n",
    "stop_words = stop_words.split(', ')\n",
    "\n",
    "def classify(val):\n",
    "    if val > 10: return 1\n",
    "    return 0\n",
    "\n",
    "X = df.drop(['score', 'contr', 'vup', 'vdo'], axis=1)\n",
    "\n",
    "#y = df_merge_art['score'].apply(classify)\n",
    "y = df['score']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, explained_variance_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "#from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Select a subset of data at a provided key.\n",
    "    key: hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    '''\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.key]\n",
    "    \n",
    "no_numbers = lambda x: re.sub(r'(\\d[\\d\\.])+', '', x.lower())\n",
    "model = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        [\n",
    "            ('statistics', Pipeline([\n",
    "                ('selector', ItemSelector(\n",
    "                    key=['weekday', 'hour', 'con_len_trans', \n",
    "                         'time_since_first_trans',\n",
    "                        'cat_schweiz', 'cat_finance', 'cat_sport', 'cat_wissen', 'cat_ausland',\n",
    "                         'cat_panorama', 'cat_community', 'cat_people', 'cat_digital'\n",
    "                        ])),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('polynomialfeatures', PolynomialFeatures(degree=3))\n",
    "\n",
    "            ]))#,\n",
    "            # add more features here\n",
    "            \n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('anova', SelectPercentile(f_regression, percentile=50)),\n",
    "    \n",
    "    # Params not optimized yet\n",
    "    #('model', ElasticNet(alpha=1e-1, l1_ratio=0.3, max_iter=50))\n",
    "#    ('model', Ridge(alpha=1e3, max_iter=10, solver='svd'))\n",
    "    ('model', DecisionTreeRegressor(max_depth=3))\n",
    "    #('model', SVC(kernel='linear', n_jobs=-1))\n",
    "])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R^2: %1.3f\" % r2_score(y_test, y_pred))\n",
    "print(\"Explained var: {:3f}\".format(explained_variance_score(y_test, y_pred)))\n",
    "print(y_pred)\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Residual plot\n",
    "X_res = X_test['time_since_first_trans']\n",
    "plt.scatter(X_res, y_test, color='black')\n",
    "plt.scatter(X_res, y_pred, color='red')\n",
    "plt.xlabel('time_since_first_trans')\n",
    "plt.show()\n",
    "\n",
    "if False:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_grid = {#'polynomialfeatures__degree': [2, 3, 4, 5]\n",
    "        'model__max_depth': list(range(1, 20)),\n",
    "#        'model__criterion': ['mse', 'friedman_mse', 'mae']\n",
    "#        'model__alpha': np.logspace(-1, 4, 6),\n",
    "#        'model__l1_ratio': [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "#        'model__max_iter': [10, 50, 100, 500],\n",
    "#        'model__solver': ['auto', 'svd', 'cholesky', 'lsqr']\n",
    "        #'anova__percentile': [5, 10, 20, 40, 60]\n",
    "                 }\n",
    "    grid = GridSearchCV(model, param_grid, cv=3)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best estimator:\\n{}\".format(grid.best_estimator_.named_steps['model']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary so far\n",
    "The decision tree regressor depends a lot on max_depth parameter: Depending on it, the model over- or underfits. I reached the best R^2 score with max_depth=3, but model underfits.\n",
    "\n",
    "So either the model can't learn the features, or the data is not good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
