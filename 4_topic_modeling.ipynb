{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships between words\n",
    "### n-grams and correlations\n",
    "Read: http://tidytextmining.com/ngrams.html\n",
    "\n",
    "Exploring relationships and connections between words.\n",
    "1. Tokenizing by n-gram, here by bigrams for a start\n",
    "2. Counting and filtering n-grams: Most common bigrams, then remove the ones where at least one is a stop word.\n",
    "3. Analyzing bigrams: one bigram per row: look at the tf-idf (the one with the highest), can be visualized for each document(/book in example).\n",
    "4. Using them to provide context in sentiment analysis: The approach is to count certain word segments like \"happy\" and \"not happy\". We examine how often sentiment-associated words are preceded by \"not\" or other negating words. We use the `AFINN lexicon for sentiment analysis`. Is there something similar in german for sentiment analysis? -> http://www.ulliwaltinger.de/sentiment/ (gives negative, positive and neutral words in tsv-format, not sure if the words have a \"sentiment score\")\n",
    "5. Calculate sentiment score per comment\n",
    "6. Visualize a network of bigrams: node1 = word1 -> node2 = word2 of bigram, weight given by the number of occurence of the bigram. For each word we only show the words that follows it the most as connected directed node. Only show bigrams that occured at least x times.  \n",
    "An useful and flexibel way to visualize relational data. - This is a visualization of a Markov chain.\n",
    "7. Put the whole thing built so far into a function for usage on other texts\n",
    "8. We may be interested in words that tend to co-occur within particular documents or particular chapters, even if they don't occur next to each other. Turn text into a wide matrix first for that.  \n",
    "Counting and correlating among sections: Package like `widyr` in R for that? (`pairwise_count`)\n",
    "9. Most common co-occuring words not that meaningful since they're also the most common individual words. We may instead want to examine correlation among words, which indicates how often they appear together relative to how often they appear separately (phi coefficient ~ pearson correlation for binary data).  \n",
    "Pick some interesting words and look at their correlations. Then plot a graph for highest correlations like above.\n",
    "    \n",
    "Pairs of consecutive words might capture structure that isn't present when one is just counting single words, and may provide context that makes tokens more understandable (for example \"pulteney street\" instead of \"pulteney\" only). However, the per-bigram counts are also sparser since they are rarer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize libraries and data\n",
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df_art = pd.read_csv('articles_2017_08.csv')\n",
    "df_com = pd.read_csv('comments_2017_08.csv').sample(30000) # crop because battery life, skews data\n",
    "# Make float better readable\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "df_com['score'] = df_com['vup'] - df_com['vdo']\n",
    "df_com['contr'] = df_com['vup'] + df_com['vdo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEFCAYAAAABjYvXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw4CIiAD8f2wVcXMVhThFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118d00b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors with 1 message: 0.392\n",
      "Authors with 2 messages: 0.054\n"
     ]
    }
   ],
   "source": [
    "# Explore relationship between authors and their number of messages\n",
    "aut = df_com.groupby('aut').size()\n",
    "\n",
    "v = aut.value_counts()\n",
    "ax = sns.barplot(x=np.array(v.keys()), y=v.values, palette=\"BuGn_d\")\n",
    "ax.set_yscale('log')  # set_yscale is a function, not a string\n",
    "n = 5\n",
    "ticks = ax.xaxis.get_ticklocs()\n",
    "ticklabels = [l.get_text() for l in ax.xaxis.get_ticklabels()]\n",
    "ax.xaxis.set_ticks(ticks[::n])\n",
    "ax.xaxis.set_ticklabels(ticklabels[::n])\n",
    "plt.ylabel('Number of authors [log]')\n",
    "plt.xlabel('Number of messages')\n",
    "plt.show()\n",
    "\n",
    "print(\"Authors with 1 message: {:.3f}\".format(v.iloc[0] / aut.sum()))\n",
    "print(\"Authors with 2 messages: {:.3f}\".format(v.iloc[1] / aut.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Topic modeling with comments\n",
    "Read: http://tidytextmining.com/topicmodeling.html\n",
    "\n",
    "Use LDA: Each document a mixture of topics and each topic a mixture of words. Built in package?\n",
    "\n",
    "It's possible to declare what a \"document\" is. Also look at how each document is classified (per topic). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=30000 and n_features=50...\n",
      "\n",
      "Topics in LDA model:\n",
      "#0: gibt alles bin andere immer keine jahren besser nie geht dies mal tun kein genau wäre einfach viele müssen heute damit leben soll menschen wohl will leute gar selber also \n",
      "\n",
      "#1: sollte jeder sondern gar kein viele wer viel machen schweizer leute bin immer einfach schweiz weil soll gibt nie geht leben damit andere keine ob also menschen besser tun mal \n",
      "\n",
      "#2: viel jetzt leute macht will mal andere einfach bin gibt sicher kommt damit viele ob soll gut leben geld kein wer besser immer tun machen geht also genau etwas würde \n",
      "\n",
      "#3: geht etwas menschen wohl heute besser damit mal keine will machen leben sollte viel einfach wer viele bin sicher soll andere anderen tun kein würde jahren wäre immer also weil \n",
      "\n",
      "#4: einfach machen geld ob sicher damit keine dies viel mal würde weil andere also alles soll sondern wohl schweiz nie viele wurde kein sollte etwas geht schweizer wer tun leute \n",
      "\n",
      "#5: gut weil müssen dies selber andere viele damit menschen jahren keine mal anderen schweizer jetzt immer würde kein sondern genau leben tun leute macht geht wäre machen einfach also wurde \n",
      "\n",
      "#6: keine genau wer soll tun schweizer alles müssen damit einfach viele menschen kein will andere nie mal leben kommt dies also etwas würde weil machen ob leute geht wäre wohl \n",
      "\n",
      "#7: immer also nie jahren wurde mal heute viel keine dies geht jetzt einfach wer sondern andere tun müssen wohl menschen damit viele kein etwas leben weil würde besser machen bin \n",
      "\n",
      "#8: mal würde wäre kein leben viele damit machen gibt will keine dies sicher andere also tun nie menschen besser wer etwas soll leute heute wohl immer wurde macht jahren einfach \n",
      "\n",
      "#9: schweiz kommt anderen viele keine mal also immer jahren wohl sicher ob gut machen alles schweizer damit kein menschen tun nie jeder gibt andere besser leben etwas sollte wäre wer \n",
      "\n",
      "\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# Parameter selection is intuitive so far\n",
    "n_samples = 30000\n",
    "n_features = 50\n",
    "n_components = 10 # Topics\n",
    "n_top_words = 30\n",
    "VERBOSE = True\n",
    "\n",
    "# https://de.wikipedia.org/wiki/Liste_der_h%C3%A4ufigsten_W%C3%B6rter_der_deutschen_Sprache\n",
    "stop_words = \"die, der, und, in, zu, den, das, nicht, von, sie, ist, des, sich, mit, dem, dass, er, es, ein, ich, auf, so, eine, auch, als, an, nach, wie, im, für, \"\n",
    "stop_words += \"man, aber, aus, durch, wenn, nur, war, noch, werden, bei, hat, wir, was, wird, sein, einen, welche, sind, oder, zur, um, haben, einer, mir, über, ihm, diese, einem, ihr, uns, \"\n",
    "stop_words += \"da, zum, kann, doch, vor, dieser, mich, ihn, du, hatte, seine, mehr, am, denn, nun, unter, sehr, selbst, schon, hier, \"\n",
    "stop_words += \"bis, habe, ihre, dann, ihnen, seiner, alle, wieder, meine, Zeit, gegen, vom, ganz, einzelnen, wo, muss, ohne, eines, können, sei, \"\n",
    "stop_words += \"nichts\"\n",
    "stop_words = stop_words.lower()\n",
    "stop_words = stop_words.split(', ')\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"#%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message, '\\n')\n",
    "    print()\n",
    "\n",
    "data_samples = df_com['con']\n",
    "#data_samples = df_art[df_art['text'].notnull()]['text']\n",
    "\n",
    "# tf-idf features for NMF (non-negative matrix factorization)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words=stop_words)\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "# term frequency (tf) for features for LDA\n",
    "tf_vectorizer = CountVectorizer(max_df=0.1, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stop_words)\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "# Fit the NMF model\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\"\"\"\n",
    "if VERBOSE: print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "if VERBOSE: print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "if VERBOSE: print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', \n",
    "          max_iter=1000, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "if VERBOSE: print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "if VERBOSE: print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\"\"\"\n",
    "if VERBOSE: print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "\n",
    "if VERBOSE: print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "if VERBOSE: print_top_words(lda, tf_feature_names, n_top_words)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of comments\thigh: 22534\tlow: 7466\n"
     ]
    }
   ],
   "source": [
    "# For each category of 20M see how the topics are \"arranged\"\n",
    "# For classes (score </>= threshold) how often each topic occurs in each comment\n",
    "threshold = 1\n",
    "topic_cols = list(map(lambda x: 'topic_' + str(x), range(0, n_components)))\n",
    "\n",
    "len_high = len(df_com[df_com['score'] >= threshold])\n",
    "len_low = len(df_com) - len_high\n",
    "print(\"Num of comments\\thigh: {}\\tlow: {}\".format(len_high, len_low))\n",
    "\n",
    "# Probability for each document to belong to a topic\n",
    "def get_probs_for_topics(text, tf_vectorizer, lda):\n",
    "    lf = lda.transform(tf_vectorizer.transform([text])).ravel()\n",
    "    return pd.Series(lf)\n",
    "\n",
    "t = df_com.apply(lambda x: get_probs_for_topics(x['con'], tf_vectorizer, lda), axis=1)\n",
    "t.columns = topic_cols\n",
    "df_com = df_com.join(t, lsuffix='_left')\n",
    "\n",
    "# Compute sums of topics per class\n",
    "s = df_com[df_com['score'] >= threshold][topic_cols].sum(axis=0) / len_high\n",
    "sums = pd.DataFrame(columns=['class', 'prob'])\n",
    "sums['prob'] = s\n",
    "sums['class'] = 'high'\n",
    "\n",
    "s = df_com[df_com['score'] < threshold][topic_cols].sum(axis=0) / len_low\n",
    "sums2 = pd.DataFrame(columns=['class', 'prob'])\n",
    "sums2['prob'] = s\n",
    "sums2['class'] = 'low'\n",
    "sums = sums.append(sums2)\n",
    "\n",
    "#print(sums)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sums_high.index)\n",
    "#print(sums_high[0])\n",
    "ax = sns.barplot(x=sums.index, y=sums['prob'], hue=sums['class'])\n",
    "plt.title('Topic occurence per class (threshold: {})'.format(threshold))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, how is the distribution of comments\n",
    "# = distribution in the columns topic_0, topic_1, ...\n",
    "column = []\n",
    "for i in range(0, n_components):\n",
    "    column.append('topic_' + str(i))\n",
    "axes = df_com.hist(column=column)\n",
    "plt.setp(axes, xticks=[]) # Remove xtick labels\n",
    "plt.subplots_adjust(hspace=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where do the comments lie compared to the topics on a cartesian plane\n",
    "import random\n",
    "from math import floor, sqrt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Show for this topics\n",
    "components_list = [0, 1, 2]\n",
    "\n",
    "for t in components_list:\n",
    "    G.add_node(t, color='red')\n",
    "    \n",
    "# Only for a subset because of computation time\n",
    "n_graph_nodes = 2000\n",
    "for i, row in df_com[:n_graph_nodes].iterrows():\n",
    "    add_weighted = []\n",
    "    G.add_node(i, color='black')\n",
    "    for t in components_list:\n",
    "        w = row['topic_' + str(t)]\n",
    "        add_weighted.append((i, t, w))\n",
    "        \n",
    "    G.add_weighted_edges_from(add_weighted)\n",
    "    \n",
    "nodes = G.nodes()\n",
    "colors = [G.node[n]['color'] for n in nodes]\n",
    "size_by_color = {'red': 300, 'black': 1}\n",
    "sizes = [size_by_color[G.node[n]['color']] for n in nodes]\n",
    "    \n",
    "fixedpos = {0:(0,0), 1:(1,0), 2: (0.5, sqrt(3/4))}\n",
    "plt.figure(figsize=(10, 10))\n",
    "pos = nx.spring_layout(G, fixed = fixedpos.keys(), pos = fixedpos)\n",
    "nx.draw_networkx(G, pos=pos, with_labels=False, width=0, node_color=colors, node_size=sizes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
